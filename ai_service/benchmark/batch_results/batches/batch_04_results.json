[
  {
    "id": "q61",
    "model": "deepseek-v3",
    "query": "MobileNet V2 微调前冻结了多少层？",
    "answer": "根据参考资料，MobileNet V2微调前冻结了前100层（共155层）。具体说明见内容3：“前100层仍设置为无法训练”。",
    "retrieve_time": 0.3058187961578369,
    "generate_time": 4.786506414413452,
    "total_time": 5.092325210571289,
    "tokens_out": 34,
    "tokens_in": 189,
    "ground_truth": "前 100 层",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q62",
    "model": "deepseek-v3",
    "query": "微调 MobileNet V2 后，验证准确率约提升到多少？",
    "answer": "根据参考资料，微调MobileNet V2基础模型的最后几层并训练分类器后，模型精度几乎达到98%（内容2）。但需注意验证损失显著高于训练损失，可能存在过拟合现象。",
    "retrieve_time": 0.190415620803833,
    "generate_time": 6.2387855052948,
    "total_time": 6.429201126098633,
    "tokens_out": 44,
    "tokens_in": 197,
    "ground_truth": "约 98%",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "经过微调后，模型精度几乎达到98%，当微调MobileNet V2基础模型的最后几层并在其上训练分类器时，验证损失远远高于训练损失，模型可能有一些过度拟合。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第382段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q63",
    "model": "deepseek-v3",
    "query": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是什么？",
    "answer": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时，常用的版本占位符是 `+`。具体代码示例如下：  \n`'org.tensorflow:tensorflow-lite:+'`  \n\n（依据参考资料内容 2 直接提供）",
    "retrieve_time": 0.26377415657043457,
    "generate_time": 6.039153337478638,
    "total_time": 6.302927494049072,
    "tokens_out": 56,
    "tokens_in": 166,
    "ground_truth": "0.0.0-nightly",
    "context": [
      {
        "content": "要使用tensorflow lite需要导入对应的库，这里通过修改build.gradle来实现。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第413段"
      },
      {
        "content": "在dependencies下增加'org.tensorflow:tensorflow-lite:+'，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第414段"
      },
      {
        "content": "8.1 认识TensorFlow Lite",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第21段"
      },
      {
        "content": "在android下增加 aaptOptions，以防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件。代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第418段"
      }
    ]
  },
  {
    "id": "q64",
    "model": "deepseek-v3",
    "query": "ImageProcessor 预处理流水线中用于裁剪或填充至方形的操作类名是什么？",
    "answer": "根据参考资料，用于裁剪或填充至方形的操作类名是 `ResizeWithCropOrPadOp`。",
    "retrieve_time": 0.25425076484680176,
    "generate_time": 4.4002907276153564,
    "total_time": 4.654541492462158,
    "tokens_out": 25,
    "tokens_in": 142,
    "ground_truth": "ResizeWithCropOrPadOp",
    "context": [
      {
        "content": "ImageProcessor imageProcessor =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第499段"
      },
      {
        "content": ".add(new ResizeWithCropOrPadOp(cropSize, cropSize))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第501段"
      },
      {
        "content": ".add(new ResizeOp(imageSizeX, imageSizeY, ResizeMethod.NEAREST_NEIGHBOR))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第502段"
      },
      {
        "content": "new ImageProcessor.Builder()",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第500段"
      }
    ]
  },
  {
    "id": "q65",
    "model": "deepseek-v3",
    "query": "TensorFlow Lite 推理结果常通过哪种数据结构（键值映射）保存标签概率？",
    "answer": "根据参考资料，TensorFlow Lite 推理结果常通过 **`Map<String, Float>`** 数据结构保存标签概率。这种键值映射将类别标签（String）与其对应的概率值（Float）关联起来，例如在内容3中明确提到的代码示例：  \n```java\nMap<String, Float> labeledProbability\n```  \n此外，内容1提到支持库的 `getTopKProbability(..)` 方法可进一步处理这种映射以提取高概率标签。  \n\n（注：若需更详细的操作方法，可参考支持库的 `TensorLabel` 工具类，如内容3的注释所示。）",
    "retrieve_time": 0.16037750244140625,
    "generate_time": 9.202543020248413,
    "total_time": 9.36292052268982,
    "tokens_out": 129,
    "tokens_in": 183,
    "ground_truth": "Map<String, Float>",
    "context": [
      {
        "content": "labeledProbability是将每个类别映射到其概率的对象。 TensorFlow Lite支持库提供了一个方便的实用程序，可将模型输出转换为概率图，使用getTopKProbability（..）方法从labeledProbability中提取前几名最可能的标签。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第526段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "// TODO: Use TensorLabel from TFLite Support Library to associate the probabilities with category labels\n    Map<String, Float> labeledProbability =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第521段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q66",
    "model": "deepseek-v3",
    "query": "示例应用中使用 GPU 推理时，需要将 GpuDelegate 添加到哪个对象？",
    "answer": "根据提供的参考资料，使用 GPU 推理时需要将 `GpuDelegate` 添加到 `tfliteOptions` 对象中。具体通过调用 `tfliteOptions.addDelegate(gpuDelegate)` 实现（参考内容 2）。",
    "retrieve_time": 0.5961298942565918,
    "generate_time": 5.993237018585205,
    "total_time": 6.589366912841797,
    "tokens_out": 49,
    "tokens_in": 129,
    "ground_truth": "Interpreter.Options",
    "context": [
      {
        "content": "GpuDelegate delegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第192段"
      },
      {
        "content": "tfliteOptions.addDelegate(gpuDelegate);",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第479段"
      },
      {
        "content": "gpuDelegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第478段"
      },
      {
        "content": "private GpuDelegate gpuDelegate = null;",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第464段"
      }
    ]
  },
  {
    "id": "q67",
    "model": "deepseek-v3",
    "query": "在 TFLite 推理过程中，哪个类代表了输入/输出张量的包装？",
    "answer": "根据提供的参考资料，没有明确提到具体代表输入/输出张量包装的类名称。资料中仅涉及TensorFlow Lite的整体架构和Classifier类的构造函数，但未提及相关张量包装类的细节。\n\n建议查阅TensorFlow Lite官方文档中关于Interpreter或Tensor类的说明以获取准确信息。",
    "retrieve_time": 0.25624656677246094,
    "generate_time": 6.326353311538696,
    "total_time": 6.582599878311157,
    "tokens_out": 63,
    "tokens_in": 128,
    "ground_truth": "TensorImage (或 TensorBuffer)",
    "context": [
      {
        "content": "在Classifier类构造函数中创建tflite实例。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第472段"
      },
      {
        "content": "8.2.1 TensorFlow Lite整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第40段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q68",
    "model": "deepseek-v3",
    "query": "为了在 build.gradle 中使用国内镜像，需要替换哪两个 Maven 仓库？",
    "answer": "根据参考资料，在 `build.gradle` 中需要替换的两个 Maven 仓库是：\n\n1. `google()`\n2. `jcenter()`\n\n其中，`jcenter()` 可替换为阿里云镜像地址：  \n`maven { url 'https://maven.aliyun.com/repository/jcenter' }`",
    "retrieve_time": 0.25623226165771484,
    "generate_time": 6.540808916091919,
    "total_time": 6.797041177749634,
    "tokens_out": 70,
    "tokens_in": 172,
    "ground_truth": "google() 和 jcenter()",
    "context": [
      {
        "content": "因为获取SDK和gradle编译环境等资源，需要先给Android Studio配置proxy或者使用国内的镜像。可将 build.gradle 中的maven源 google() 和 jcenter() 分别替换为国内镜像，如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第425段"
      },
      {
        "content": "能配置build.gradle构建项目",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第18段"
      },
      {
        "content": "配置build.gradle",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第411段"
      },
      {
        "content": "maven { url ' https://maven.aliyun.com/repository/jcenter '}",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第429段"
      }
    ]
  },
  {
    "id": "q69",
    "model": "deepseek-v3",
    "query": "TensorFlow Lite 提供的 Android 示例项目中，花卉识别代码所在的 Git 路径是什么？",
    "answer": "根据提供的参考资料，花卉识别代码的路径位于 `examples/lite/codelabs/flower_classification/android/` 目录下。其中：\n- `start/` 是项目模板\n- `finish/` 是完整代码",
    "retrieve_time": 0.27340054512023926,
    "generate_time": 5.734070539474487,
    "total_time": 6.007471084594727,
    "tokens_out": 50,
    "tokens_in": 223,
    "ground_truth": "examples/lite/codelabs/flower_classification/android/",
    "context": [
      {
        "content": "项目代码位于目录examples/lite/codelabs/flower_classification/android/，start目录下为项目模板，finish目录下是项目完整代码。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第402段"
      },
      {
        "content": "工作目录中选择 examples/lite/codelabs/flower_classification/android/finish。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第409段"
      },
      {
        "content": "我们已经使用MobileNet V2 创建、训练和导出了自定义TensorFlow Lite模型，已经导出以下经过训练的TF Lite模型文件和标签文件。接下来将在手机端部署，运行一个使用该模型识别花卉图片的Android 应用。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第398段"
      },
      {
        "content": "下面将使用TensorFlow Lite实现花卉识别app，在Android设备上运行图像识别模型MobileNets_v2来识别花卉。本项目实施步骤如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第216段"
      }
    ]
  },
  {
    "id": "q70",
    "model": "deepseek-v3",
    "query": "ImageDataGenerator 在示例中进行了哪两种主要功能？",
    "answer": "根据参考资料，ImageDataGenerator 在示例中主要实现了以下两种功能：\n\n1. **像素缩放**（通过构造函数参数对图像数据进行归一化等操作）\n2. **数据增强**（通过参数配置实现图像变换以扩充训练数据）\n\n（依据内容1和内容4总结得出）",
    "retrieve_time": 0.2720799446105957,
    "generate_time": 6.5517072677612305,
    "total_time": 6.823787212371826,
    "tokens_out": 62,
    "tokens_in": 197,
    "ground_truth": "像素归一化 (rescale) 与验证集拆分 (validation_split)",
    "context": [
      {
        "content": "ImageDataGenerator的构造函数包含许多参数，用于指定加载后如何操作图像数据，包括像素缩放和数据增强。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第254段"
      },
      {
        "content": "Image Data Generator的构造罕述",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 186.4-189.0s"
      },
      {
        "content": "Kirons提供了Image Data Generator类",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 169.6-173.0s"
      },
      {
        "content": "将数据集划分为训练集和验证集。训练前需要手动加载图像数据，完成包括遍历数据集的目录结构、加载图像数据以及返回输入和输出。可以使用 Keras 提供的 ImageDataGenerator 类，它是keras.preprocessing.image模块中的图片生成器，负责生成一个批次一个批次的图片，以生成器的形式给模型训练",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第253段"
      }
    ]
  },
  {
    "id": "q71",
    "model": "deepseek-v3",
    "query": "在训练花卉模型时，使用了多少个 epoch？",
    "answer": "根据提供的参考资料，没有提及训练花卉模型时使用的具体 epoch 数量。因此，我无法回答这个问题。",
    "retrieve_time": 0.2259664535522461,
    "generate_time": 4.750776290893555,
    "total_time": 4.976742744445801,
    "tokens_out": 23,
    "tokens_in": 123,
    "ground_truth": "10",
    "context": [
      {
        "content": "通过迁移学习实现花卉识别模型",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第217段"
      },
      {
        "content": "我们将使用带有标注的花会数据集",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 91.6-94.8s"
      },
      {
        "content": "8.4 任务2：实现花卉识别",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第215段"
      },
      {
        "content": "在训练之前先编译模型，损失函数使用类别交叉熵。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第320段"
      }
    ]
  },
  {
    "id": "q72",
    "model": "deepseek-v3",
    "query": "微调阶段额外训练的 epoch 数被设置为多少？",
    "answer": "根据参考资料中的内容4，微调阶段额外训练的epoch数被设置为5。",
    "retrieve_time": 0.15236425399780273,
    "generate_time": 4.212320327758789,
    "total_time": 4.364684581756592,
    "tokens_out": 17,
    "tokens_in": 186,
    "ground_truth": "5",
    "context": [
      {
        "content": "通过微调进一步提高性能，调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征，当训练数据集很大并且非常类似于预训练模型训练的原始数据集时，通常建议使用此技术。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第227段"
      },
      {
        "content": "fine_tune_at = 100",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第362段"
      },
      {
        "content": "由于是通过迁移学习改造模型，所以不改变基础模型的各项参数变量，因为这样才能保留原来大规模训练的优势。使用model.trainable = False，设置在训练中，基础模型的各项参数变量不会被新的训练修改数据。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第305段"
      },
      {
        "content": "epochs=5,",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第379段"
      }
    ]
  },
  {
    "id": "q73",
    "model": "deepseek-v3",
    "query": "MobileNet V2 微调前的非可训练参数主要来源于什么？",
    "answer": "MobileNet V2微调前的非可训练参数主要来源于其预训练的基础模型（base_model）的权重。根据参考资料，当设置`model.trainable = False`时，预训练网络的权重在训练期间不会被更新，这些冻结的权重即为非可训练参数。具体来说，这些参数是在ImageNet等大型数据集上预训练得到的，包含了卷积层、批量归一化层等结构的权重（如资料提到的155层网络中的前100层）。",
    "retrieve_time": 0.18369269371032715,
    "generate_time": 8.011772394180298,
    "total_time": 8.195465087890625,
    "tokens_out": 98,
    "tokens_in": 193,
    "ground_truth": "预训练的 ImageNet 权重",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      }
    ]
  },
  {
    "id": "q74",
    "model": "deepseek-v3",
    "query": "Android 应用中，若要使用 GPU 加速，需要声明哪种权限或设置？",
    "answer": "根据参考资料，在Android应用中使用GPU加速需要以下设置：  \n1. **启用开发者模式**和**USB调试**（内容3），这是通过Android Studio部署调试应用的基础条件。  \n2. **硬件加速支持**（内容4）：需确保应用或特定组件（如View、WebView）启用了硬件加速，通常在AndroidManifest.xml中声明`android:hardwareAccelerated=\"true\"`（全局或Activity级别）。  \n\n无需额外声明权限，但需注意设备GPU兼容性。若资料未提及具体权限，则无明确权限要求。",
    "retrieve_time": 0.2652449607849121,
    "generate_time": 8.866432905197144,
    "total_time": 9.131677865982056,
    "tokens_out": 122,
    "tokens_in": 185,
    "ground_truth": "无需额外权限，只需在代码中添加 GpuDelegate",
    "context": [
      {
        "content": "如果手机有GPU， GPU 比 CPU 执行更快的浮点矩阵运算，速度提升能有显著效果。例如在有GPU加速的手机上运行MobileNet图像分类，模型运行速度可以提高 5.5 倍。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第189段"
      },
      {
        "content": "利用手机GPU加速模型运行",
        "source": "1112_222613.mp4 109.5-115.2s"
      },
      {
        "content": "如图在Android 设备上运行，设置手机启用“开发者模式”和“USB 调试”，否则无法将该应用从 Android Studio 加载到您的手机上。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第540段"
      },
      {
        "content": "3) 能够利用各种硬件加速。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第30段"
      }
    ]
  },
  {
    "id": "q75",
    "model": "deepseek-v3",
    "query": "PoseNet Android 示例仓库的完整 GitHub URL 是什么？",
    "answer": "PoseNet Android 示例仓库的完整 GitHub URL 是：  \nhttps://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "retrieve_time": 0.26053333282470703,
    "generate_time": 5.158451080322266,
    "total_time": 5.418984413146973,
    "tokens_out": 35,
    "tokens_in": 203,
    "ground_truth": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "context": [
      {
        "content": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第553段"
      },
      {
        "content": "从 PoseNet 库中调用estimateSinglePose()函数来获取Person对象。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第559段"
      },
      {
        "content": "该PoseNet示例应用程序功能是捕捉摄像头拍摄的帧，并实时覆盖图像上的关键点。应用程序对每张传入的摄像头图像执行以下操作：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第556段"
      },
      {
        "content": "通过PoseNet模型实现人体姿势估计，PoseNet可以通过检测关键身体部位的位置来估计图像或者视频中的人体姿势。例如，该模型可以估计图像中人的手肘和/或膝盖位置。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第551段"
      }
    ]
  },
  {
    "id": "q76",
    "model": "deepseek-v3",
    "query": "TensorFlow.js 提供哪两大 API 层以构建模型？",
    "answer": "根据参考资料，TensorFlow.js 提供以下两大 API 层以构建模型：\n\n1. Kore API  \n2. Layer API  \n\n此外，也可以通过 `TF.model` 来创建基于 Layer API 的模型（如内容2、3所述）。",
    "retrieve_time": 0.30675268173217773,
    "generate_time": 7.143123388290405,
    "total_time": 7.449876070022583,
    "tokens_out": 51,
    "tokens_in": 138,
    "ground_truth": "Core API 与 Layers API",
    "context": [
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "也可以通过TF.model来创建Layer's model。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 63.5-68.6s"
      },
      {
        "content": "以下是使用TF.model API建立和上文相同模型的例子。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 74.2-81.7s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q77",
    "model": "deepseek-v3",
    "query": "在浏览器环境中，TensorFlow.js 主要依赖哪项技术进行 GPU 加速？",
    "answer": "根据参考资料，TensorFlow.js（用户问题中可能误写为\"Tenzeflow.js\"）在浏览器环境中主要依赖 **GPU硬件加速** 技术实现性能优化（参考内容2）。具体实现通常基于WebGL API，但参考资料中未明确提及此项细节。",
    "retrieve_time": 1.5012831687927246,
    "generate_time": 6.392441272735596,
    "total_time": 7.89372444152832,
    "tokens_out": 59,
    "tokens_in": 145,
    "ground_truth": "WebGL",
    "context": [
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q78",
    "model": "deepseek-v3",
    "query": "在 Node.js 环境中，若安装 CUDA，TensorFlow.js 可使用哪种后端进行加速？",
    "answer": "在 Node.js 环境中，若已安装 CUDA 环境支持，TensorFlow.js 可以使用 **GPU 硬件加速**作为后端进行加速（基于内容 2 和内容 3）。但参考资料未明确提及具体后端名称（如 \"tensorflow\" 或 \"node-gpu\"），因此无法进一步确认技术细节。",
    "retrieve_time": 0.2983860969543457,
    "generate_time": 6.879955053329468,
    "total_time": 7.1783411502838135,
    "tokens_out": 71,
    "tokens_in": 143,
    "ground_truth": "CUDA GPU 后端",
    "context": [
      {
        "content": "在Note.js 环境中如果有CUDA 环境支持",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 89.0-94.0s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q79",
    "model": "deepseek-v3",
    "query": "若要在 HTML 中直接加载 TensorFlow.js，应使用哪个标签引入脚本？",
    "answer": "在HTML中直接加载TensorFlow.js应使用`<script>`标签引入。具体方法可参考NPM发布的包或CDN链接（例如：`<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\"></script>`）。根据资料1和2，这是最方便的浏览器加载方式。",
    "retrieve_time": 0.2696225643157959,
    "generate_time": 6.8190391063690186,
    "total_time": 7.0886616706848145,
    "tokens_out": 73,
    "tokens_in": 160,
    "ground_truth": "<script> 标签",
    "context": [
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Index.html 文件中,使用脚本代码在流暖器项目中或许Tensorflow.js 以及TFGS 当VAS。",
        "source": "upload-17308567376682724494-06 手写数字识别-1_174128.mp4 234.8-245.8s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      }
    ]
  },
  {
    "id": "q80",
    "model": "deepseek-v3",
    "query": "TensorFlow.js 张量的两个核心属性分别是什么？",
    "answer": "根据提供的参考资料，无法找到关于TensorFlow.js张量核心属性的相关信息。因此，我不知道答案。",
    "retrieve_time": 0.2713441848754883,
    "generate_time": 4.517893075942993,
    "total_time": 4.7892372608184814,
    "tokens_out": 21,
    "tokens_in": 143,
    "ground_truth": "shape（形状）和 rank（阶/维度）",
    "context": [
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q61",
    "model": "qwen-7b-chat",
    "query": "MobileNet V2 微调前冻结了多少层？",
    "answer": "根据提供的资料，当微调MobileNet V2时，前100层被设置为不可训练（即被冻结），而总共有155层。这意味着在开始微调过程之初，有55层是可训练的状态。因此，答案是：MobileNet V2微调前冻结了100层。",
    "retrieve_time": 0.29506802558898926,
    "generate_time": 7.8510942459106445,
    "total_time": 8.146162271499634,
    "tokens_out": 72,
    "tokens_in": 207,
    "ground_truth": "前 100 层",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q62",
    "model": "qwen-7b-chat",
    "query": "微调 MobileNet V2 后，验证准确率约提升到多少？",
    "answer": "根据提供的资料，经过微调后，MobileNet V2模型的精度几乎达到了98%。这意味着验证准确率大约提升到了98%左右。不过，请注意实际结果可能会因具体应用场景、数据集特性等因素有所不同。",
    "retrieve_time": 0.28065943717956543,
    "generate_time": 3.3447556495666504,
    "total_time": 3.625415086746216,
    "tokens_out": 50,
    "tokens_in": 212,
    "ground_truth": "约 98%",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "经过微调后，模型精度几乎达到98%，当微调MobileNet V2基础模型的最后几层并在其上训练分类器时，验证损失远远高于训练损失，模型可能有一些过度拟合。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第382段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q63",
    "model": "qwen-7b-chat",
    "query": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是什么？",
    "answer": "在 Android 的 `build.gradle` 文件中添加 TensorFlow Lite 依赖时，常用的版本占位符是 `+`。这表示使用最新版本的库。例如，您可以这样写：\n\n```gradle\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:+'\n}\n```\n\n但是，请注意，虽然使用 `+` 可以方便地获取最新版本，但为了确保项目的稳定性和兼容性，建议明确指定一个具体的版本号。",
    "retrieve_time": 0.15363311767578125,
    "generate_time": 6.710513353347778,
    "total_time": 6.86414647102356,
    "tokens_out": 95,
    "tokens_in": 166,
    "ground_truth": "0.0.0-nightly",
    "context": [
      {
        "content": "要使用tensorflow lite需要导入对应的库，这里通过修改build.gradle来实现。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第413段"
      },
      {
        "content": "在dependencies下增加'org.tensorflow:tensorflow-lite:+'，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第414段"
      },
      {
        "content": "8.1 认识TensorFlow Lite",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第21段"
      },
      {
        "content": "在android下增加 aaptOptions，以防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件。代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第418段"
      }
    ]
  },
  {
    "id": "q64",
    "model": "qwen-7b-chat",
    "query": "ImageProcessor 预处理流水线中用于裁剪或填充至方形的操作类名是什么？",
    "answer": "在ImageProcessor预处理流水线中，用于裁剪或填充至方形的操作类名是`ResizeWithCropOrPadOp`。",
    "retrieve_time": 0.46949005126953125,
    "generate_time": 1.3861761093139648,
    "total_time": 1.855666160583496,
    "tokens_out": 29,
    "tokens_in": 141,
    "ground_truth": "ResizeWithCropOrPadOp",
    "context": [
      {
        "content": "ImageProcessor imageProcessor =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第499段"
      },
      {
        "content": ".add(new ResizeWithCropOrPadOp(cropSize, cropSize))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第501段"
      },
      {
        "content": ".add(new ResizeOp(imageSizeX, imageSizeY, ResizeMethod.NEAREST_NEIGHBOR))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第502段"
      },
      {
        "content": "new ImageProcessor.Builder()",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第500段"
      }
    ]
  },
  {
    "id": "q65",
    "model": "qwen-7b-chat",
    "query": "TensorFlow Lite 推理结果常通过哪种数据结构（键值映射）保存标签概率？",
    "answer": "TensorFlow Lite 推理结果常通过`Map<String, Float>`这种键值映射的数据结构来保存标签及其对应的概率。这里，`String`作为键表示类别标签，而`Float`则代表该类别的预测概率。这样的数据结构允许用户轻松地将每个预测出的类别与它所对应的可能性关联起来。在实际应用中，通常会使用TensorFlow Lite支持库中的工具或方法（如`getTopKProbability`）来处理这些概率数据，从而获取最有可能的几个标签。",
    "retrieve_time": 0.22111296653747559,
    "generate_time": 6.619507074356079,
    "total_time": 6.840620040893555,
    "tokens_out": 113,
    "tokens_in": 196,
    "ground_truth": "Map<String, Float>",
    "context": [
      {
        "content": "labeledProbability是将每个类别映射到其概率的对象。 TensorFlow Lite支持库提供了一个方便的实用程序，可将模型输出转换为概率图，使用getTopKProbability（..）方法从labeledProbability中提取前几名最可能的标签。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第526段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "// TODO: Use TensorLabel from TFLite Support Library to associate the probabilities with category labels\n    Map<String, Float> labeledProbability =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第521段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q66",
    "model": "qwen-7b-chat",
    "query": "示例应用中使用 GPU 推理时，需要将 GpuDelegate 添加到哪个对象？",
    "answer": "根据提供的参考资料，`GpuDelegate` 需要被添加到 `tfliteOptions` 对象中。具体来说，是通过调用 `tfliteOptions.addDelegate(gpuDelegate);` 方法实现的。这表明，在准备使用TensorFlow Lite进行GPU加速推理前，你需要创建一个`GpuDelegate`实例，并将其添加至你的`tfliteOptions`配置对象里。",
    "retrieve_time": 0.2828652858734131,
    "generate_time": 5.509738206863403,
    "total_time": 5.792603492736816,
    "tokens_out": 87,
    "tokens_in": 132,
    "ground_truth": "Interpreter.Options",
    "context": [
      {
        "content": "GpuDelegate delegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第192段"
      },
      {
        "content": "tfliteOptions.addDelegate(gpuDelegate);",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第479段"
      },
      {
        "content": "gpuDelegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第478段"
      },
      {
        "content": "private GpuDelegate gpuDelegate = null;",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第464段"
      }
    ]
  },
  {
    "id": "q67",
    "model": "qwen-7b-chat",
    "query": "在 TFLite 推理过程中，哪个类代表了输入/输出张量的包装？",
    "answer": "根据提供的参考资料，没有直接提到哪个类专门用来代表TFLite推理过程中输入/输出张量的包装。在TensorFlow Lite的实际应用中，通常会使用`Interpreter`类来加载模型并处理输入/输出张量的操作，但具体的“包装”类如`Tensor`等，并未在给定资料中明确指出。因此，基于现有资料无法给出确切答案。对于更详细的API用法或特定类的功能，建议查阅TensorFlow Lite官方文档获取最准确的信息。",
    "retrieve_time": 0.3265693187713623,
    "generate_time": 7.2483861446380615,
    "total_time": 7.574955463409424,
    "tokens_out": 106,
    "tokens_in": 133,
    "ground_truth": "TensorImage (或 TensorBuffer)",
    "context": [
      {
        "content": "在Classifier类构造函数中创建tflite实例。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第472段"
      },
      {
        "content": "8.2.1 TensorFlow Lite整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第40段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q68",
    "model": "qwen-7b-chat",
    "query": "为了在 build.gradle 中使用国内镜像，需要替换哪两个 Maven 仓库？",
    "answer": "为了在 `build.gradle` 中使用国内镜像，您需要将 Maven 仓库 `google()` 和 `jcenter()` 替换为相应的国内镜像。根据提供的资料，至少提到了一个替换方案，即用阿里云的 jcenter 镜像来替换 `jcenter()`：\n\n```gradle\nmaven { url 'https://maven.aliyun.com/repository/jcenter' }\n```\n\n对于 `google()` 的替换，虽然直接信息没有给出具体的国内镜像地址，但通常做法是也找一个可靠的国内镜像源进行替换。不过基于给定的信息，我们仅能确定 `jcenter()` 应被上述阿里云链接所替代。关于 `google()` 的具体国内镜像地址未在参考资料中明确指出。",
    "retrieve_time": 0.23349714279174805,
    "generate_time": 11.076860427856445,
    "total_time": 11.310357570648193,
    "tokens_out": 161,
    "tokens_in": 172,
    "ground_truth": "google() 和 jcenter()",
    "context": [
      {
        "content": "因为获取SDK和gradle编译环境等资源，需要先给Android Studio配置proxy或者使用国内的镜像。可将 build.gradle 中的maven源 google() 和 jcenter() 分别替换为国内镜像，如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第425段"
      },
      {
        "content": "能配置build.gradle构建项目",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第18段"
      },
      {
        "content": "配置build.gradle",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第411段"
      },
      {
        "content": "maven { url ' https://maven.aliyun.com/repository/jcenter '}",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第429段"
      }
    ]
  },
  {
    "id": "q69",
    "model": "qwen-7b-chat",
    "query": "TensorFlow Lite 提供的 Android 示例项目中，花卉识别代码所在的 Git 路径是什么？",
    "answer": "根据提供的参考资料，花卉识别代码所在的目录是 `examples/lite/codelabs/flower_classification/android/`。但是，具体的Git仓库URL或路径没有在给定的信息中提到。因此，对于完整的Git路径，我无法从当前的资料中给出确切的答案。如果你需要这个信息，建议查看项目的文档或联系项目管理员获取更详细的指引。",
    "retrieve_time": 0.275362491607666,
    "generate_time": 6.046577453613281,
    "total_time": 6.321939945220947,
    "tokens_out": 76,
    "tokens_in": 232,
    "ground_truth": "examples/lite/codelabs/flower_classification/android/",
    "context": [
      {
        "content": "项目代码位于目录examples/lite/codelabs/flower_classification/android/，start目录下为项目模板，finish目录下是项目完整代码。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第402段"
      },
      {
        "content": "工作目录中选择 examples/lite/codelabs/flower_classification/android/finish。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第409段"
      },
      {
        "content": "我们已经使用MobileNet V2 创建、训练和导出了自定义TensorFlow Lite模型，已经导出以下经过训练的TF Lite模型文件和标签文件。接下来将在手机端部署，运行一个使用该模型识别花卉图片的Android 应用。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第398段"
      },
      {
        "content": "下面将使用TensorFlow Lite实现花卉识别app，在Android设备上运行图像识别模型MobileNets_v2来识别花卉。本项目实施步骤如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第216段"
      }
    ]
  },
  {
    "id": "q70",
    "model": "qwen-7b-chat",
    "query": "ImageDataGenerator 在示例中进行了哪两种主要功能？",
    "answer": "根据提供的参考资料，`ImageDataGenerator` 主要执行了两个功能：\n1. **数据增强**：通过应用一系列随机变换（如旋转、平移等）来增加训练集的多样性，从而帮助提高模型的泛化能力。\n2. **批量生成图像数据**：以生成器的形式为模型提供一个批次接一个批次的图片，这样可以更有效地利用内存资源，尤其是在处理大量或大型图像时。\n\n这两个功能使得`ImageDataGenerator`成为处理图像数据集时非常有用的工具。",
    "retrieve_time": 0.24119329452514648,
    "generate_time": 6.32094669342041,
    "total_time": 6.562139987945557,
    "tokens_out": 108,
    "tokens_in": 208,
    "ground_truth": "像素归一化 (rescale) 与验证集拆分 (validation_split)",
    "context": [
      {
        "content": "ImageDataGenerator的构造函数包含许多参数，用于指定加载后如何操作图像数据，包括像素缩放和数据增强。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第254段"
      },
      {
        "content": "Image Data Generator的构造罕述",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 186.4-189.0s"
      },
      {
        "content": "Kirons提供了Image Data Generator类",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 169.6-173.0s"
      },
      {
        "content": "将数据集划分为训练集和验证集。训练前需要手动加载图像数据，完成包括遍历数据集的目录结构、加载图像数据以及返回输入和输出。可以使用 Keras 提供的 ImageDataGenerator 类，它是keras.preprocessing.image模块中的图片生成器，负责生成一个批次一个批次的图片，以生成器的形式给模型训练",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第253段"
      }
    ]
  },
  {
    "id": "q71",
    "model": "qwen-7b-chat",
    "query": "在训练花卉模型时，使用了多少个 epoch？",
    "answer": "根据提供的参考资料，没有明确提到在训练花卉识别模型时使用了多少个epoch。因此，对于您的问题，我无法给出具体的答案。如果您有更多关于训练设置的信息或者需要建议一个合适的epoch数量，请提供或询问进一步的细节。",
    "retrieve_time": 0.26008081436157227,
    "generate_time": 2.722238540649414,
    "total_time": 2.9823193550109863,
    "tokens_out": 52,
    "tokens_in": 130,
    "ground_truth": "10",
    "context": [
      {
        "content": "通过迁移学习实现花卉识别模型",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第217段"
      },
      {
        "content": "我们将使用带有标注的花会数据集",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 91.6-94.8s"
      },
      {
        "content": "8.4 任务2：实现花卉识别",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第215段"
      },
      {
        "content": "在训练之前先编译模型，损失函数使用类别交叉熵。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第320段"
      }
    ]
  },
  {
    "id": "q72",
    "model": "qwen-7b-chat",
    "query": "微调阶段额外训练的 epoch 数被设置为多少？",
    "answer": "根据提供的参考资料，微调阶段额外训练的 epoch 数被设置为 5。",
    "retrieve_time": 0.399200439453125,
    "generate_time": 2.2723734378814697,
    "total_time": 2.6715738773345947,
    "tokens_out": 18,
    "tokens_in": 202,
    "ground_truth": "5",
    "context": [
      {
        "content": "通过微调进一步提高性能，调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征，当训练数据集很大并且非常类似于预训练模型训练的原始数据集时，通常建议使用此技术。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第227段"
      },
      {
        "content": "fine_tune_at = 100",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第362段"
      },
      {
        "content": "由于是通过迁移学习改造模型，所以不改变基础模型的各项参数变量，因为这样才能保留原来大规模训练的优势。使用model.trainable = False，设置在训练中，基础模型的各项参数变量不会被新的训练修改数据。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第305段"
      },
      {
        "content": "epochs=5,",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第379段"
      }
    ]
  },
  {
    "id": "q73",
    "model": "qwen-7b-chat",
    "query": "MobileNet V2 微调前的非可训练参数主要来源于什么？",
    "answer": "MobileNet V2 微调前的非可训练参数主要来源于预训练模型。这些参数是通过在大规模数据集上预先训练得到的，当设置 `model.trainable = False` 时，意味着这些预训练权重在后续特定任务（如添加新的分类层）的训练过程中将保持不变，即不会被更新。这样做可以利用预训练模型已经学到的特征表示能力来加速新任务的学习过程或提高其性能，同时减少对大量标记数据的需求。",
    "retrieve_time": 0.15838122367858887,
    "generate_time": 7.760598182678223,
    "total_time": 7.9189794063568115,
    "tokens_out": 104,
    "tokens_in": 212,
    "ground_truth": "预训练的 ImageNet 权重",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      }
    ]
  },
  {
    "id": "q74",
    "model": "qwen-7b-chat",
    "query": "Android 应用中，若要使用 GPU 加速，需要声明哪种权限或设置？",
    "answer": "根据提供的参考资料，并没有直接提到在Android应用中使用GPU加速需要声明的具体权限。但是，为了能够有效地开发和测试利用了GPU加速的应用程序，资料指出需要在手机上启用“开发者模式”和“USB 调试”。这一步骤主要是为了让开发者能够更方便地将应用程序从Android Studio加载到手机上进行测试，并不直接关联于应用内部的权限设置。\n\n对于实际开发过程中，是否需要在`AndroidManifest.xml`文件或其他地方明确声明特殊权限来启用GPU加速功能，给定的信息里并没有给出答案。一般而言，基本的OpenGL ES或Vulkan API调用来实现GPU渲染并不需要额外的权限声明，但具体需求可能依赖于所使用的库或者框架的要求。若要确保正确性，建议查阅相关API文档或是你计划集成的具体库/SDK的官方指南。",
    "retrieve_time": 0.2763099670410156,
    "generate_time": 12.867275953292847,
    "total_time": 13.143585920333862,
    "tokens_out": 178,
    "tokens_in": 204,
    "ground_truth": "无需额外权限，只需在代码中添加 GpuDelegate",
    "context": [
      {
        "content": "如果手机有GPU， GPU 比 CPU 执行更快的浮点矩阵运算，速度提升能有显著效果。例如在有GPU加速的手机上运行MobileNet图像分类，模型运行速度可以提高 5.5 倍。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第189段"
      },
      {
        "content": "利用手机GPU加速模型运行",
        "source": "1112_222613.mp4 109.5-115.2s"
      },
      {
        "content": "如图在Android 设备上运行，设置手机启用“开发者模式”和“USB 调试”，否则无法将该应用从 Android Studio 加载到您的手机上。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第540段"
      },
      {
        "content": "3) 能够利用各种硬件加速。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第30段"
      }
    ]
  },
  {
    "id": "q75",
    "model": "qwen-7b-chat",
    "query": "PoseNet Android 示例仓库的完整 GitHub URL 是什么？",
    "answer": "PoseNet Android 示例仓库的完整 GitHub URL 是: https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "retrieve_time": 1.4731886386871338,
    "generate_time": 1.8222413063049316,
    "total_time": 3.2954299449920654,
    "tokens_out": 28,
    "tokens_in": 201,
    "ground_truth": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "context": [
      {
        "content": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第553段"
      },
      {
        "content": "从 PoseNet 库中调用estimateSinglePose()函数来获取Person对象。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第559段"
      },
      {
        "content": "该PoseNet示例应用程序功能是捕捉摄像头拍摄的帧，并实时覆盖图像上的关键点。应用程序对每张传入的摄像头图像执行以下操作：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第556段"
      },
      {
        "content": "通过PoseNet模型实现人体姿势估计，PoseNet可以通过检测关键身体部位的位置来估计图像或者视频中的人体姿势。例如，该模型可以估计图像中人的手肘和/或膝盖位置。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第551段"
      }
    ]
  },
  {
    "id": "q76",
    "model": "qwen-7b-chat",
    "query": "TensorFlow.js 提供哪两大 API 层以构建模型？",
    "answer": "TensorFlow.js 提供了Kore API与Layer API两大API层以构建模型。不过，这里似乎有一个小的拼写错误，“Kore API”应该是指“Core API”。因此，正确的答案是TensorFlow.js提供了Core API与Layer API两大API层来构建模型。",
    "retrieve_time": 0.22765517234802246,
    "generate_time": 3.2552707195281982,
    "total_time": 3.4829258918762207,
    "tokens_out": 61,
    "tokens_in": 143,
    "ground_truth": "Core API 与 Layers API",
    "context": [
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "也可以通过TF.model来创建Layer's model。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 63.5-68.6s"
      },
      {
        "content": "以下是使用TF.model API建立和上文相同模型的例子。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 74.2-81.7s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q77",
    "model": "qwen-7b-chat",
    "query": "在浏览器环境中，TensorFlow.js 主要依赖哪项技术进行 GPU 加速？",
    "answer": "在浏览器环境中，TensorFlow.js 主要依赖WebGL技术进行GPU加速。不过，请注意给定的参考资料中并没有直接提到这一点；根据资料内容2，我们知道TensorFlow.js支持GPU硬件加速，但具体的技术细节（即WebGL）未在提供的资料中明确指出。通常情况下，对于基于浏览器的应用来说，WebGL是实现这种加速的常见方式。",
    "retrieve_time": 0.26995277404785156,
    "generate_time": 4.578268051147461,
    "total_time": 4.8482208251953125,
    "tokens_out": 80,
    "tokens_in": 152,
    "ground_truth": "WebGL",
    "context": [
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q78",
    "model": "qwen-7b-chat",
    "query": "在 Node.js 环境中，若安装 CUDA，TensorFlow.js 可使用哪种后端进行加速？",
    "answer": "在 Node.js 环境中，如果安装了 CUDA，TensorFlow.js 可以使用 GPU 作为后端进行加速。这意味着它能够利用硬件（具体来说是支持CUDA的NVIDIA GPU）来提升计算性能。",
    "retrieve_time": 0.1713705062866211,
    "generate_time": 3.3119397163391113,
    "total_time": 3.4833102226257324,
    "tokens_out": 50,
    "tokens_in": 157,
    "ground_truth": "CUDA GPU 后端",
    "context": [
      {
        "content": "在Note.js 环境中如果有CUDA 环境支持",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 89.0-94.0s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q79",
    "model": "qwen-7b-chat",
    "query": "若要在 HTML 中直接加载 TensorFlow.js，应使用哪个标签引入脚本？",
    "answer": "要在 HTML 中直接加载 TensorFlow.js，应该使用 `<script>` 标签来引入脚本。根据提供的资料，虽然没有明确写出具体的 URL 或方法，但通常的做法是这样的：\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\"></script>\n```\n\n这里用到了CDN（内容分发网络）链接来加载 TensorFlow.js 库。如果你有特定版本的需求，可以将上述URL中的`@latest`替换为你需要的具体版本号。\n\n注意：参考资料中提到的“Tenzeflow.js”可能是对“TensorFlow.js”的拼写错误。确保在实际操作时使用正确的库名。",
    "retrieve_time": 0.18792271614074707,
    "generate_time": 6.134511709213257,
    "total_time": 6.322434425354004,
    "tokens_out": 137,
    "tokens_in": 165,
    "ground_truth": "<script> 标签",
    "context": [
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Index.html 文件中,使用脚本代码在流暖器项目中或许Tensorflow.js 以及TFGS 当VAS。",
        "source": "upload-17308567376682724494-06 手写数字识别-1_174128.mp4 234.8-245.8s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      }
    ]
  },
  {
    "id": "q80",
    "model": "qwen-7b-chat",
    "query": "TensorFlow.js 张量的两个核心属性分别是什么？",
    "answer": "根据提供的参考资料，没有直接提到TensorFlow.js中张量的两个核心属性。因此，基于现有资料我无法给出准确答案。对于这个问题的具体内容，建议参考TensorFlow.js官方文档或相关学习材料获取更详细的信息。",
    "retrieve_time": 0.22588586807250977,
    "generate_time": 2.684915065765381,
    "total_time": 2.9108009338378906,
    "tokens_out": 50,
    "tokens_in": 149,
    "ground_truth": "shape（形状）和 rank（阶/维度）",
    "context": [
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q61",
    "model": "chatglm3-6b",
    "query": "MobileNet V2 微调前冻结了多少层？",
    "answer": "MobileNet V2微调前冻结了前100层，共计155层。",
    "retrieve_time": 0.27168965339660645,
    "generate_time": 1.038560390472412,
    "total_time": 1.3102500438690186,
    "tokens_out": 19,
    "tokens_in": 190,
    "ground_truth": "前 100 层",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q62",
    "model": "chatglm3-6b",
    "query": "微调 MobileNet V2 后，验证准确率约提升到多少？",
    "answer": "微调MobileNet V2后，验证准确率几乎达到98%。",
    "retrieve_time": 0.14722943305969238,
    "generate_time": 0.9708936214447021,
    "total_time": 1.1181230545043945,
    "tokens_out": 17,
    "tokens_in": 197,
    "ground_truth": "约 98%",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "经过微调后，模型精度几乎达到98%，当微调MobileNet V2基础模型的最后几层并在其上训练分类器时，验证损失远远高于训练损失，模型可能有一些过度拟合。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第382段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q63",
    "model": "chatglm3-6b",
    "query": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是什么？",
    "answer": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是 `+` ，它表示使用最新的可用版本。例如：\n\n```gradle\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:+'\n}\n```",
    "retrieve_time": 0.17780709266662598,
    "generate_time": 2.07942271232605,
    "total_time": 2.257229804992676,
    "tokens_out": 52,
    "tokens_in": 160,
    "ground_truth": "0.0.0-nightly",
    "context": [
      {
        "content": "要使用tensorflow lite需要导入对应的库，这里通过修改build.gradle来实现。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第413段"
      },
      {
        "content": "在dependencies下增加'org.tensorflow:tensorflow-lite:+'，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第414段"
      },
      {
        "content": "8.1 认识TensorFlow Lite",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第21段"
      },
      {
        "content": "在android下增加 aaptOptions，以防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件。代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第418段"
      }
    ]
  },
  {
    "id": "q64",
    "model": "chatglm3-6b",
    "query": "ImageProcessor 预处理流水线中用于裁剪或填充至方形的操作类名是什么？",
    "answer": "操作类名是 `ResizeWithCropOrPadOp`。",
    "retrieve_time": 0.182358980178833,
    "generate_time": 0.8037662506103516,
    "total_time": 0.9861252307891846,
    "tokens_out": 15,
    "tokens_in": 138,
    "ground_truth": "ResizeWithCropOrPadOp",
    "context": [
      {
        "content": "ImageProcessor imageProcessor =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第499段"
      },
      {
        "content": ".add(new ResizeWithCropOrPadOp(cropSize, cropSize))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第501段"
      },
      {
        "content": ".add(new ResizeOp(imageSizeX, imageSizeY, ResizeMethod.NEAREST_NEIGHBOR))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第502段"
      },
      {
        "content": "new ImageProcessor.Builder()",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第500段"
      }
    ]
  },
  {
    "id": "q65",
    "model": "chatglm3-6b",
    "query": "TensorFlow Lite 推理结果常通过哪种数据结构（键值映射）保存标签概率？",
    "answer": "TensorFlow Lite 推理结果通常通过`Map<String, Float>`这种数据结构保存标签概率，其中字符串表示类别标签，浮点数表示相应类别的概率。",
    "retrieve_time": 0.24740314483642578,
    "generate_time": 1.5699191093444824,
    "total_time": 1.8173222541809082,
    "tokens_out": 39,
    "tokens_in": 186,
    "ground_truth": "Map<String, Float>",
    "context": [
      {
        "content": "labeledProbability是将每个类别映射到其概率的对象。 TensorFlow Lite支持库提供了一个方便的实用程序，可将模型输出转换为概率图，使用getTopKProbability（..）方法从labeledProbability中提取前几名最可能的标签。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第526段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "// TODO: Use TensorLabel from TFLite Support Library to associate the probabilities with category labels\n    Map<String, Float> labeledProbability =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第521段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q66",
    "model": "chatglm3-6b",
    "query": "示例应用中使用 GPU 推理时，需要将 GpuDelegate 添加到哪个对象？",
    "answer": "需要将 GpuDelegate 添加到 `tfliteOptions` 对象中。如参考资料中所示，通过 `tfliteOptions.addDelegate(gpuDelegate)` 这行代码来完成这个操作。",
    "retrieve_time": 0.2008070945739746,
    "generate_time": 1.6681733131408691,
    "total_time": 1.8689804077148438,
    "tokens_out": 44,
    "tokens_in": 130,
    "ground_truth": "Interpreter.Options",
    "context": [
      {
        "content": "GpuDelegate delegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第192段"
      },
      {
        "content": "tfliteOptions.addDelegate(gpuDelegate);",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第479段"
      },
      {
        "content": "gpuDelegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第478段"
      },
      {
        "content": "private GpuDelegate gpuDelegate = null;",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第464段"
      }
    ]
  },
  {
    "id": "q67",
    "model": "chatglm3-6b",
    "query": "在 TFLite 推理过程中，哪个类代表了输入/输出张量的包装？",
    "answer": "不知道。参考资料中没有提供关于TFLite推理过程中代表输入/输出张量包装的类信息。",
    "retrieve_time": 0.1527860164642334,
    "generate_time": 1.021486759185791,
    "total_time": 1.1742727756500244,
    "tokens_out": 24,
    "tokens_in": 127,
    "ground_truth": "TensorImage (或 TensorBuffer)",
    "context": [
      {
        "content": "在Classifier类构造函数中创建tflite实例。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第472段"
      },
      {
        "content": "8.2.1 TensorFlow Lite整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第40段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q68",
    "model": "chatglm3-6b",
    "query": "为了在 build.gradle 中使用国内镜像，需要替换哪两个 Maven 仓库？",
    "answer": "需要替换的Maven仓库是 google() 和 jcenter()。可以将它们分别替换为国内的镜像源。参考资料中提供了一个例子，即替换为阿里云的jcenter镜像：maven { url 'https://maven.aliyun.com/repository/jcenter' }。但没有提供google()的具体替换源，通常也可以选择国内其他可靠的镜像服务。",
    "retrieve_time": 0.15273404121398926,
    "generate_time": 2.6281349658966064,
    "total_time": 2.7808690071105957,
    "tokens_out": 75,
    "tokens_in": 160,
    "ground_truth": "google() 和 jcenter()",
    "context": [
      {
        "content": "因为获取SDK和gradle编译环境等资源，需要先给Android Studio配置proxy或者使用国内的镜像。可将 build.gradle 中的maven源 google() 和 jcenter() 分别替换为国内镜像，如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第425段"
      },
      {
        "content": "能配置build.gradle构建项目",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第18段"
      },
      {
        "content": "配置build.gradle",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第411段"
      },
      {
        "content": "maven { url ' https://maven.aliyun.com/repository/jcenter '}",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第429段"
      }
    ]
  },
  {
    "id": "q69",
    "model": "chatglm3-6b",
    "query": "TensorFlow Lite 提供的 Android 示例项目中，花卉识别代码所在的 Git 路径是什么？",
    "answer": "`examples/lite/codelabs/flower_classification/android/`",
    "retrieve_time": 0.15446758270263672,
    "generate_time": 0.8270261287689209,
    "total_time": 0.9814937114715576,
    "tokens_out": 15,
    "tokens_in": 225,
    "ground_truth": "examples/lite/codelabs/flower_classification/android/",
    "context": [
      {
        "content": "项目代码位于目录examples/lite/codelabs/flower_classification/android/，start目录下为项目模板，finish目录下是项目完整代码。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第402段"
      },
      {
        "content": "工作目录中选择 examples/lite/codelabs/flower_classification/android/finish。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第409段"
      },
      {
        "content": "我们已经使用MobileNet V2 创建、训练和导出了自定义TensorFlow Lite模型，已经导出以下经过训练的TF Lite模型文件和标签文件。接下来将在手机端部署，运行一个使用该模型识别花卉图片的Android 应用。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第398段"
      },
      {
        "content": "下面将使用TensorFlow Lite实现花卉识别app，在Android设备上运行图像识别模型MobileNets_v2来识别花卉。本项目实施步骤如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第216段"
      }
    ]
  },
  {
    "id": "q70",
    "model": "chatglm3-6b",
    "query": "ImageDataGenerator 在示例中进行了哪两种主要功能？",
    "answer": "根据提供的参考资料，ImageDataGenerator进行了以下两种主要功能：\n\n1. 像素缩放：调整图像数据的像素值，可能包括标准化等操作。\n2. 数据增强：在训练过程中对图像数据进行增强变换，如旋转、翻转等，以扩充数据集并提高模型泛化能力。",
    "retrieve_time": 0.15929365158081055,
    "generate_time": 2.3949344158172607,
    "total_time": 2.5542280673980713,
    "tokens_out": 66,
    "tokens_in": 201,
    "ground_truth": "像素归一化 (rescale) 与验证集拆分 (validation_split)",
    "context": [
      {
        "content": "ImageDataGenerator的构造函数包含许多参数，用于指定加载后如何操作图像数据，包括像素缩放和数据增强。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第254段"
      },
      {
        "content": "Image Data Generator的构造罕述",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 186.4-189.0s"
      },
      {
        "content": "Kirons提供了Image Data Generator类",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 169.6-173.0s"
      },
      {
        "content": "将数据集划分为训练集和验证集。训练前需要手动加载图像数据，完成包括遍历数据集的目录结构、加载图像数据以及返回输入和输出。可以使用 Keras 提供的 ImageDataGenerator 类，它是keras.preprocessing.image模块中的图片生成器，负责生成一个批次一个批次的图片，以生成器的形式给模型训练",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第253段"
      }
    ]
  },
  {
    "id": "q71",
    "model": "chatglm3-6b",
    "query": "在训练花卉模型时，使用了多少个 epoch？",
    "answer": "不知道。参考资料中没有提供关于训练花卉模型时使用了多少个epoch的信息。",
    "retrieve_time": 0.15968108177185059,
    "generate_time": 0.7938308715820312,
    "total_time": 0.9535119533538818,
    "tokens_out": 18,
    "tokens_in": 123,
    "ground_truth": "10",
    "context": [
      {
        "content": "通过迁移学习实现花卉识别模型",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第217段"
      },
      {
        "content": "我们将使用带有标注的花会数据集",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 91.6-94.8s"
      },
      {
        "content": "8.4 任务2：实现花卉识别",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第215段"
      },
      {
        "content": "在训练之前先编译模型，损失函数使用类别交叉熵。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第320段"
      }
    ]
  },
  {
    "id": "q72",
    "model": "chatglm3-6b",
    "query": "微调阶段额外训练的 epoch 数被设置为多少？",
    "answer": "微调阶段额外训练的 epoch 数被设置为5。",
    "retrieve_time": 0.1951737403869629,
    "generate_time": 0.7185580730438232,
    "total_time": 0.9137318134307861,
    "tokens_out": 14,
    "tokens_in": 189,
    "ground_truth": "5",
    "context": [
      {
        "content": "通过微调进一步提高性能，调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征，当训练数据集很大并且非常类似于预训练模型训练的原始数据集时，通常建议使用此技术。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第227段"
      },
      {
        "content": "fine_tune_at = 100",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第362段"
      },
      {
        "content": "由于是通过迁移学习改造模型，所以不改变基础模型的各项参数变量，因为这样才能保留原来大规模训练的优势。使用model.trainable = False，设置在训练中，基础模型的各项参数变量不会被新的训练修改数据。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第305段"
      },
      {
        "content": "epochs=5,",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第379段"
      }
    ]
  },
  {
    "id": "q73",
    "model": "chatglm3-6b",
    "query": "MobileNet V2 微调前的非可训练参数主要来源于什么？",
    "answer": "MobileNet V2微调前的非可训练参数主要来源于预训练网络的权重，即在设置`model.trainable = False`时，冻结了预训练模型的大部分层，通常是为了保持模型已学习到的特征提取能力。根据资料4，这里指的是前100层。这些层在微调前保持不可训练状态，以便在新的任务上仅训练少数几层或顶层。",
    "retrieve_time": 0.18104124069213867,
    "generate_time": 2.8685142993927,
    "total_time": 3.049555540084839,
    "tokens_out": 84,
    "tokens_in": 194,
    "ground_truth": "预训练的 ImageNet 权重",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      }
    ]
  },
  {
    "id": "q74",
    "model": "chatglm3-6b",
    "query": "Android 应用中，若要使用 GPU 加速，需要声明哪种权限或设置？",
    "answer": "在Android应用中，若要使用GPU加速，通常不需要声明特殊的权限。但是，需要确保开启了“开发者模式”和“USB调试”设置，这有助于从Android Studio将应用加载到手机上并进行相关的硬件加速测试。具体到GPU加速，Android系统通常会自动管理硬件资源的分配，应用可以通过OpenGL ES或Vulkan等API来请求使用GPU。但不需要特别声明权限来启用GPU加速功能。然而，如果应用直接涉及到硬件访问或者特定系统级别的操作，可能需要声明相关的权限，但这并非GPU加速所必需的。\n\n简而言之，对于一般的GPU加速使用，不需要额外声明权限，但需要确保手机的“开发者模式”和“USB调试”选项已开启。",
    "retrieve_time": 0.19712567329406738,
    "generate_time": 4.908991098403931,
    "total_time": 5.106116771697998,
    "tokens_out": 150,
    "tokens_in": 188,
    "ground_truth": "无需额外权限，只需在代码中添加 GpuDelegate",
    "context": [
      {
        "content": "如果手机有GPU， GPU 比 CPU 执行更快的浮点矩阵运算，速度提升能有显著效果。例如在有GPU加速的手机上运行MobileNet图像分类，模型运行速度可以提高 5.5 倍。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第189段"
      },
      {
        "content": "利用手机GPU加速模型运行",
        "source": "1112_222613.mp4 109.5-115.2s"
      },
      {
        "content": "如图在Android 设备上运行，设置手机启用“开发者模式”和“USB 调试”，否则无法将该应用从 Android Studio 加载到您的手机上。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第540段"
      },
      {
        "content": "3) 能够利用各种硬件加速。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第30段"
      }
    ]
  },
  {
    "id": "q75",
    "model": "chatglm3-6b",
    "query": "PoseNet Android 示例仓库的完整 GitHub URL 是什么？",
    "answer": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
    "retrieve_time": 0.14127564430236816,
    "generate_time": 0.9948976039886475,
    "total_time": 1.1361732482910156,
    "tokens_out": 20,
    "tokens_in": 195,
    "ground_truth": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "context": [
      {
        "content": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第553段"
      },
      {
        "content": "从 PoseNet 库中调用estimateSinglePose()函数来获取Person对象。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第559段"
      },
      {
        "content": "该PoseNet示例应用程序功能是捕捉摄像头拍摄的帧，并实时覆盖图像上的关键点。应用程序对每张传入的摄像头图像执行以下操作：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第556段"
      },
      {
        "content": "通过PoseNet模型实现人体姿势估计，PoseNet可以通过检测关键身体部位的位置来估计图像或者视频中的人体姿势。例如，该模型可以估计图像中人的手肘和/或膝盖位置。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第551段"
      }
    ]
  },
  {
    "id": "q76",
    "model": "chatglm3-6b",
    "query": "TensorFlow.js 提供哪两大 API 层以构建模型？",
    "answer": "TensorFlow.js 提供了 Kore API 与 Layer API 两大层以构建模型。",
    "retrieve_time": 0.17083978652954102,
    "generate_time": 0.9989919662475586,
    "total_time": 1.1698317527770996,
    "tokens_out": 21,
    "tokens_in": 137,
    "ground_truth": "Core API 与 Layers API",
    "context": [
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "也可以通过TF.model来创建Layer's model。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 63.5-68.6s"
      },
      {
        "content": "以下是使用TF.model API建立和上文相同模型的例子。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 74.2-81.7s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q77",
    "model": "chatglm3-6b",
    "query": "在浏览器环境中，TensorFlow.js 主要依赖哪项技术进行 GPU 加速？",
    "answer": "TensorFlow.js 在浏览器环境中主要依赖 WebGL 技术进行 GPU 加速。资料中提到的 Tenzeflow.js 可能是 TensorFlow.js 的误写。根据资料内容 2，Tenzeflow.js（即 TensorFlow.js）支持 GPU 硬件加速。而 WebGL 正是 TensorFlow.js 在浏览器中实现硬件加速（尤其是 GPU 加速）的关键技术。",
    "retrieve_time": 0.16932892799377441,
    "generate_time": 3.592283010482788,
    "total_time": 3.7616119384765625,
    "tokens_out": 80,
    "tokens_in": 144,
    "ground_truth": "WebGL",
    "context": [
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q78",
    "model": "chatglm3-6b",
    "query": "在 Node.js 环境中，若安装 CUDA，TensorFlow.js 可使用哪种后端进行加速？",
    "answer": "在 Node.js 环境中，如果安装了 CUDA，TensorFlow.js 可以使用 `tf-node-gpu` 后端进行加速。这是因为在有 CUDA 环境支持的情况下，Tenzeflow.js 能够利用 GPU 硬件加速。参考资料中提到 Tenzeflow.js 支持GPU硬件加速，而在 Node.js 环境下，对应的 GPU 加速后端就是 `tf-node-gpu`。",
    "retrieve_time": 0.23673534393310547,
    "generate_time": 3.5073862075805664,
    "total_time": 3.744121551513672,
    "tokens_out": 95,
    "tokens_in": 150,
    "ground_truth": "CUDA GPU 后端",
    "context": [
      {
        "content": "在Note.js 环境中如果有CUDA 环境支持",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 89.0-94.0s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q79",
    "model": "chatglm3-6b",
    "query": "若要在 HTML 中直接加载 TensorFlow.js，应使用哪个标签引入脚本？",
    "answer": "应该使用 `<script>` 标签引入 TensorFlow.js。",
    "retrieve_time": 0.22081875801086426,
    "generate_time": 0.7600162029266357,
    "total_time": 0.9808349609375,
    "tokens_out": 14,
    "tokens_in": 155,
    "ground_truth": "<script> 标签",
    "context": [
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Index.html 文件中,使用脚本代码在流暖器项目中或许Tensorflow.js 以及TFGS 当VAS。",
        "source": "upload-17308567376682724494-06 手写数字识别-1_174128.mp4 234.8-245.8s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      }
    ]
  },
  {
    "id": "q80",
    "model": "chatglm3-6b",
    "query": "TensorFlow.js 张量的两个核心属性分别是什么？",
    "answer": "不知道。参考资料中没有提供关于TensorFlow.js 张量的两个核心属性的信息。",
    "retrieve_time": 0.18337321281433105,
    "generate_time": 1.028494119644165,
    "total_time": 1.211867332458496,
    "tokens_out": 18,
    "tokens_in": 140,
    "ground_truth": "shape（形状）和 rank（阶/维度）",
    "context": [
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q61",
    "model": "baichuan2-13b-chat",
    "query": "MobileNet V2 微调前冻结了多少层？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.30468177795410156,
    "generate_time": 0.24207115173339844,
    "total_time": 0.5467529296875,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "前 100 层",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q62",
    "model": "baichuan2-13b-chat",
    "query": "微调 MobileNet V2 后，验证准确率约提升到多少？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.19629240036010742,
    "generate_time": 0.08049321174621582,
    "total_time": 0.27678561210632324,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "约 98%",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "经过微调后，模型精度几乎达到98%，当微调MobileNet V2基础模型的最后几层并在其上训练分类器时，验证损失远远高于训练损失，模型可能有一些过度拟合。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第382段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q63",
    "model": "baichuan2-13b-chat",
    "query": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是什么？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.15914463996887207,
    "generate_time": 0.14308929443359375,
    "total_time": 0.3022339344024658,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "0.0.0-nightly",
    "context": [
      {
        "content": "要使用tensorflow lite需要导入对应的库，这里通过修改build.gradle来实现。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第413段"
      },
      {
        "content": "在dependencies下增加'org.tensorflow:tensorflow-lite:+'，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第414段"
      },
      {
        "content": "8.1 认识TensorFlow Lite",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第21段"
      },
      {
        "content": "在android下增加 aaptOptions，以防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件。代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第418段"
      }
    ]
  },
  {
    "id": "q64",
    "model": "baichuan2-13b-chat",
    "query": "ImageProcessor 预处理流水线中用于裁剪或填充至方形的操作类名是什么？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.3256247043609619,
    "generate_time": 0.09050297737121582,
    "total_time": 0.41612768173217773,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "ResizeWithCropOrPadOp",
    "context": [
      {
        "content": "ImageProcessor imageProcessor =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第499段"
      },
      {
        "content": ".add(new ResizeWithCropOrPadOp(cropSize, cropSize))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第501段"
      },
      {
        "content": ".add(new ResizeOp(imageSizeX, imageSizeY, ResizeMethod.NEAREST_NEIGHBOR))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第502段"
      },
      {
        "content": "new ImageProcessor.Builder()",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第500段"
      }
    ]
  },
  {
    "id": "q65",
    "model": "baichuan2-13b-chat",
    "query": "TensorFlow Lite 推理结果常通过哪种数据结构（键值映射）保存标签概率？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.2256450653076172,
    "generate_time": 0.0917806625366211,
    "total_time": 0.3174257278442383,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "Map<String, Float>",
    "context": [
      {
        "content": "labeledProbability是将每个类别映射到其概率的对象。 TensorFlow Lite支持库提供了一个方便的实用程序，可将模型输出转换为概率图，使用getTopKProbability（..）方法从labeledProbability中提取前几名最可能的标签。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第526段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "// TODO: Use TensorLabel from TFLite Support Library to associate the probabilities with category labels\n    Map<String, Float> labeledProbability =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第521段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q66",
    "model": "baichuan2-13b-chat",
    "query": "示例应用中使用 GPU 推理时，需要将 GpuDelegate 添加到哪个对象？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.17278814315795898,
    "generate_time": 0.13323187828063965,
    "total_time": 0.30602002143859863,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "Interpreter.Options",
    "context": [
      {
        "content": "GpuDelegate delegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第192段"
      },
      {
        "content": "tfliteOptions.addDelegate(gpuDelegate);",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第479段"
      },
      {
        "content": "gpuDelegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第478段"
      },
      {
        "content": "private GpuDelegate gpuDelegate = null;",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第464段"
      }
    ]
  },
  {
    "id": "q67",
    "model": "baichuan2-13b-chat",
    "query": "在 TFLite 推理过程中，哪个类代表了输入/输出张量的包装？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.28534746170043945,
    "generate_time": 0.10070943832397461,
    "total_time": 0.38605690002441406,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "TensorImage (或 TensorBuffer)",
    "context": [
      {
        "content": "在Classifier类构造函数中创建tflite实例。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第472段"
      },
      {
        "content": "8.2.1 TensorFlow Lite整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第40段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q68",
    "model": "baichuan2-13b-chat",
    "query": "为了在 build.gradle 中使用国内镜像，需要替换哪两个 Maven 仓库？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.2214517593383789,
    "generate_time": 0.10254740715026855,
    "total_time": 0.32399916648864746,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "google() 和 jcenter()",
    "context": [
      {
        "content": "因为获取SDK和gradle编译环境等资源，需要先给Android Studio配置proxy或者使用国内的镜像。可将 build.gradle 中的maven源 google() 和 jcenter() 分别替换为国内镜像，如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第425段"
      },
      {
        "content": "能配置build.gradle构建项目",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第18段"
      },
      {
        "content": "配置build.gradle",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第411段"
      },
      {
        "content": "maven { url ' https://maven.aliyun.com/repository/jcenter '}",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第429段"
      }
    ]
  },
  {
    "id": "q69",
    "model": "baichuan2-13b-chat",
    "query": "TensorFlow Lite 提供的 Android 示例项目中，花卉识别代码所在的 Git 路径是什么？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.42037010192871094,
    "generate_time": 0.3811326026916504,
    "total_time": 0.8015027046203613,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "examples/lite/codelabs/flower_classification/android/",
    "context": [
      {
        "content": "项目代码位于目录examples/lite/codelabs/flower_classification/android/，start目录下为项目模板，finish目录下是项目完整代码。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第402段"
      },
      {
        "content": "工作目录中选择 examples/lite/codelabs/flower_classification/android/finish。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第409段"
      },
      {
        "content": "我们已经使用MobileNet V2 创建、训练和导出了自定义TensorFlow Lite模型，已经导出以下经过训练的TF Lite模型文件和标签文件。接下来将在手机端部署，运行一个使用该模型识别花卉图片的Android 应用。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第398段"
      },
      {
        "content": "下面将使用TensorFlow Lite实现花卉识别app，在Android设备上运行图像识别模型MobileNets_v2来识别花卉。本项目实施步骤如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第216段"
      }
    ]
  },
  {
    "id": "q70",
    "model": "baichuan2-13b-chat",
    "query": "ImageDataGenerator 在示例中进行了哪两种主要功能？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.28289031982421875,
    "generate_time": 0.09604740142822266,
    "total_time": 0.3789377212524414,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "像素归一化 (rescale) 与验证集拆分 (validation_split)",
    "context": [
      {
        "content": "ImageDataGenerator的构造函数包含许多参数，用于指定加载后如何操作图像数据，包括像素缩放和数据增强。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第254段"
      },
      {
        "content": "Image Data Generator的构造罕述",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 186.4-189.0s"
      },
      {
        "content": "Kirons提供了Image Data Generator类",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 169.6-173.0s"
      },
      {
        "content": "将数据集划分为训练集和验证集。训练前需要手动加载图像数据，完成包括遍历数据集的目录结构、加载图像数据以及返回输入和输出。可以使用 Keras 提供的 ImageDataGenerator 类，它是keras.preprocessing.image模块中的图片生成器，负责生成一个批次一个批次的图片，以生成器的形式给模型训练",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第253段"
      }
    ]
  },
  {
    "id": "q71",
    "model": "baichuan2-13b-chat",
    "query": "在训练花卉模型时，使用了多少个 epoch？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.2690572738647461,
    "generate_time": 0.18902826309204102,
    "total_time": 0.4580855369567871,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "10",
    "context": [
      {
        "content": "通过迁移学习实现花卉识别模型",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第217段"
      },
      {
        "content": "我们将使用带有标注的花会数据集",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 91.6-94.8s"
      },
      {
        "content": "8.4 任务2：实现花卉识别",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第215段"
      },
      {
        "content": "在训练之前先编译模型，损失函数使用类别交叉熵。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第320段"
      }
    ]
  },
  {
    "id": "q72",
    "model": "baichuan2-13b-chat",
    "query": "微调阶段额外训练的 epoch 数被设置为多少？",
    "answer": "模型调用错误: Error code: 404 - {'error': {'code': 'model_not_found', 'param': None, 'type': 'invalid_request_error', 'message': 'The model `Baichuan2-13B-Chat` does not exist'}}",
    "retrieve_time": 0.3466014862060547,
    "generate_time": 0.07324671745300293,
    "total_time": 0.4198482036590576,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "5",
    "context": [
      {
        "content": "通过微调进一步提高性能，调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征，当训练数据集很大并且非常类似于预训练模型训练的原始数据集时，通常建议使用此技术。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第227段"
      },
      {
        "content": "fine_tune_at = 100",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第362段"
      },
      {
        "content": "由于是通过迁移学习改造模型，所以不改变基础模型的各项参数变量，因为这样才能保留原来大规模训练的优势。使用model.trainable = False，设置在训练中，基础模型的各项参数变量不会被新的训练修改数据。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第305段"
      },
      {
        "content": "epochs=5,",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第379段"
      }
    ]
  },
  {
    "id": "q73",
    "model": "baichuan2-13b-chat",
    "query": "MobileNet V2 微调前的非可训练参数主要来源于什么？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.30475687980651855,
    "generate_time": 1.478468894958496,
    "total_time": 1.7832257747650146,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "预训练的 ImageNet 权重",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      }
    ]
  },
  {
    "id": "q74",
    "model": "baichuan2-13b-chat",
    "query": "Android 应用中，若要使用 GPU 加速，需要声明哪种权限或设置？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.2941420078277588,
    "generate_time": 2.0375382900238037,
    "total_time": 2.3316802978515625,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "无需额外权限，只需在代码中添加 GpuDelegate",
    "context": [
      {
        "content": "如果手机有GPU， GPU 比 CPU 执行更快的浮点矩阵运算，速度提升能有显著效果。例如在有GPU加速的手机上运行MobileNet图像分类，模型运行速度可以提高 5.5 倍。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第189段"
      },
      {
        "content": "利用手机GPU加速模型运行",
        "source": "1112_222613.mp4 109.5-115.2s"
      },
      {
        "content": "如图在Android 设备上运行，设置手机启用“开发者模式”和“USB 调试”，否则无法将该应用从 Android Studio 加载到您的手机上。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第540段"
      },
      {
        "content": "3) 能够利用各种硬件加速。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第30段"
      }
    ]
  },
  {
    "id": "q75",
    "model": "baichuan2-13b-chat",
    "query": "PoseNet Android 示例仓库的完整 GitHub URL 是什么？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.284085750579834,
    "generate_time": 1.488102674484253,
    "total_time": 1.772188425064087,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "context": [
      {
        "content": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第553段"
      },
      {
        "content": "从 PoseNet 库中调用estimateSinglePose()函数来获取Person对象。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第559段"
      },
      {
        "content": "该PoseNet示例应用程序功能是捕捉摄像头拍摄的帧，并实时覆盖图像上的关键点。应用程序对每张传入的摄像头图像执行以下操作：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第556段"
      },
      {
        "content": "通过PoseNet模型实现人体姿势估计，PoseNet可以通过检测关键身体部位的位置来估计图像或者视频中的人体姿势。例如，该模型可以估计图像中人的手肘和/或膝盖位置。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第551段"
      }
    ]
  },
  {
    "id": "q76",
    "model": "baichuan2-13b-chat",
    "query": "TensorFlow.js 提供哪两大 API 层以构建模型？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.3207728862762451,
    "generate_time": 1.6455838680267334,
    "total_time": 1.9663567543029785,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "Core API 与 Layers API",
    "context": [
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "也可以通过TF.model来创建Layer's model。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 63.5-68.6s"
      },
      {
        "content": "以下是使用TF.model API建立和上文相同模型的例子。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 74.2-81.7s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q77",
    "model": "baichuan2-13b-chat",
    "query": "在浏览器环境中，TensorFlow.js 主要依赖哪项技术进行 GPU 加速？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.2807803153991699,
    "generate_time": 1.4729015827178955,
    "total_time": 1.7536818981170654,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "WebGL",
    "context": [
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q78",
    "model": "baichuan2-13b-chat",
    "query": "在 Node.js 环境中，若安装 CUDA，TensorFlow.js 可使用哪种后端进行加速？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.27760815620422363,
    "generate_time": 1.8910560607910156,
    "total_time": 2.1686642169952393,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "CUDA GPU 后端",
    "context": [
      {
        "content": "在Note.js 环境中如果有CUDA 环境支持",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 89.0-94.0s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q79",
    "model": "baichuan2-13b-chat",
    "query": "若要在 HTML 中直接加载 TensorFlow.js，应使用哪个标签引入脚本？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.28600430488586426,
    "generate_time": 1.5971791744232178,
    "total_time": 1.883183479309082,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "<script> 标签",
    "context": [
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Index.html 文件中,使用脚本代码在流暖器项目中或许Tensorflow.js 以及TFGS 当VAS。",
        "source": "upload-17308567376682724494-06 手写数字识别-1_174128.mp4 234.8-245.8s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      }
    ]
  },
  {
    "id": "q80",
    "model": "baichuan2-13b-chat",
    "query": "TensorFlow.js 张量的两个核心属性分别是什么？",
    "answer": "模型调用错误: Error code: 429 - {'error': {'code': 'rate_limit', 'param': None, 'type': 'rate_limit_error', 'message': 'Request too frequent, please try again later'}}",
    "retrieve_time": 0.24693083763122559,
    "generate_time": 2.947751045227051,
    "total_time": 3.1946818828582764,
    "tokens_out": 0,
    "tokens_in": 0,
    "ground_truth": "shape（形状）和 rank（阶/维度）",
    "context": [
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q61",
    "model": "yi-6b-chat",
    "query": "MobileNet V2 微调前冻结了多少层？",
    "answer": "根据提供的资料，在微调MobileNet V2时，前100层被设置为无法训练。这意味着在取消冻结之前，这些层是冻结的。\n\n回答: 在微调前，MobileNet V2冻结了100层。",
    "retrieve_time": 0.44557785987854004,
    "generate_time": 2.359862804412842,
    "total_time": 2.805440664291382,
    "tokens_out": 60,
    "tokens_in": 228,
    "ground_truth": "前 100 层",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q62",
    "model": "yi-6b-chat",
    "query": "微调 MobileNet V2 后，验证准确率约提升到多少？",
    "answer": "根据提供的资料，微调MobileNet V2后，模型精度几乎达到98%。因此，微调后验证准确率大约提升到98%。\n\n回答: 微调后验证准确率大约提升到98%。",
    "retrieve_time": 0.19125795364379883,
    "generate_time": 2.291348457336426,
    "total_time": 2.4826064109802246,
    "tokens_out": 51,
    "tokens_in": 231,
    "ground_truth": "约 98%",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "经过微调后，模型精度几乎达到98%，当微调MobileNet V2基础模型的最后几层并在其上训练分类器时，验证损失远远高于训练损失，模型可能有一些过度拟合。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第382段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      }
    ]
  },
  {
    "id": "q63",
    "model": "yi-6b-chat",
    "query": "在 Android build.gradle 中添加 TensorFlow Lite 依赖时常用的版本占位符是什么？",
    "answer": "在 Android 的 `build.gradle` 中添加 TensorFlow Lite 依赖时，常用的版本占位符是 `'+'`，这将自动获取 TensorFlow Lite 库的最新版本。\n\n```gradle\ndependencies {\n    implementation 'org.tensorflow:tensorflow-lite:+'\n}\n```\n\n这个占位符 `'+'` 可以在构建时获取最新的可用版本。",
    "retrieve_time": 0.19367122650146484,
    "generate_time": 3.105389356613159,
    "total_time": 3.299060583114624,
    "tokens_out": 97,
    "tokens_in": 202,
    "ground_truth": "0.0.0-nightly",
    "context": [
      {
        "content": "要使用tensorflow lite需要导入对应的库，这里通过修改build.gradle来实现。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第413段"
      },
      {
        "content": "在dependencies下增加'org.tensorflow:tensorflow-lite:+'，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第414段"
      },
      {
        "content": "8.1 认识TensorFlow Lite",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第21段"
      },
      {
        "content": "在android下增加 aaptOptions，以防止Android在生成应用程序二进制文件时压缩TensorFlow Lite模型文件。代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第418段"
      }
    ]
  },
  {
    "id": "q64",
    "model": "yi-6b-chat",
    "query": "ImageProcessor 预处理流水线中用于裁剪或填充至方形的操作类名是什么？",
    "answer": "用于裁剪或填充至方形的操作类名是 **ResizeWithCropOrPadOp**。\n\n回答: ResizeWithCropOrPadOp",
    "retrieve_time": 0.14565539360046387,
    "generate_time": 1.8898141384124756,
    "total_time": 2.0354695320129395,
    "tokens_out": 37,
    "tokens_in": 181,
    "ground_truth": "ResizeWithCropOrPadOp",
    "context": [
      {
        "content": "ImageProcessor imageProcessor =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第499段"
      },
      {
        "content": ".add(new ResizeWithCropOrPadOp(cropSize, cropSize))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第501段"
      },
      {
        "content": ".add(new ResizeOp(imageSizeX, imageSizeY, ResizeMethod.NEAREST_NEIGHBOR))",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第502段"
      },
      {
        "content": "new ImageProcessor.Builder()",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第500段"
      }
    ]
  },
  {
    "id": "q65",
    "model": "yi-6b-chat",
    "query": "TensorFlow Lite 推理结果常通过哪种数据结构（键值映射）保存标签概率？",
    "answer": "TensorFlow Lite 推理结果常通过键值映射的数据结构保存标签概率，具体使用的是 `Map<String, Float>` 类型的 `labeledProbability` 对象。该对象将每个类别标签映射到其对应的概率。",
    "retrieve_time": 0.15322375297546387,
    "generate_time": 1.9223206043243408,
    "total_time": 2.0755443572998047,
    "tokens_out": 56,
    "tokens_in": 231,
    "ground_truth": "Map<String, Float>",
    "context": [
      {
        "content": "labeledProbability是将每个类别映射到其概率的对象。 TensorFlow Lite支持库提供了一个方便的实用程序，可将模型输出转换为概率图，使用getTopKProbability（..）方法从labeledProbability中提取前几名最可能的标签。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第526段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "// TODO: Use TensorLabel from TFLite Support Library to associate the probabilities with category labels\n    Map<String, Float> labeledProbability =",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第521段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q66",
    "model": "yi-6b-chat",
    "query": "示例应用中使用 GPU 推理时，需要将 GpuDelegate 添加到哪个对象？",
    "answer": "根据提供的参考资料，示例应用中使用 GPU 推理时，需要将 **GpuDelegate** 添加到 **tfliteOptions** 对象。\n\n回答: 需要将 GpuDelegate 添加到 **tfliteOptions** 对象。",
    "retrieve_time": 0.14526891708374023,
    "generate_time": 2.330691337585449,
    "total_time": 2.4759602546691895,
    "tokens_out": 59,
    "tokens_in": 168,
    "ground_truth": "Interpreter.Options",
    "context": [
      {
        "content": "GpuDelegate delegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第192段"
      },
      {
        "content": "tfliteOptions.addDelegate(gpuDelegate);",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第479段"
      },
      {
        "content": "gpuDelegate = new GpuDelegate();",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第478段"
      },
      {
        "content": "private GpuDelegate gpuDelegate = null;",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第464段"
      }
    ]
  },
  {
    "id": "q67",
    "model": "yi-6b-chat",
    "query": "在 TFLite 推理过程中，哪个类代表了输入/输出张量的包装？",
    "answer": "在 TensorFlow Lite 推理过程中，代表输入/输出张量的包装的类是 `Tensor` 类。该类用于处理与解释器交互的输入和输出数据。\n\n回答完毕。",
    "retrieve_time": 0.17835521697998047,
    "generate_time": 2.1971113681793213,
    "total_time": 2.3754665851593018,
    "tokens_out": 46,
    "tokens_in": 157,
    "ground_truth": "TensorImage (或 TensorBuffer)",
    "context": [
      {
        "content": "在Classifier类构造函数中创建tflite实例。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第472段"
      },
      {
        "content": "8.2.1 TensorFlow Lite整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第40段"
      },
      {
        "content": "8.2 TensorFlow Lite体系结构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第38段"
      },
      {
        "content": "掌握TensorFlow Lite的整体架构",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第10段"
      }
    ]
  },
  {
    "id": "q68",
    "model": "yi-6b-chat",
    "query": "为了在 build.gradle 中使用国内镜像，需要替换哪两个 Maven 仓库？",
    "answer": "为了在 build.gradle 中使用国内镜像，需要将 Maven 仓库 google() 和 jcenter() 分别替换为国内镜像源。根据提供的资料，可以将它们替换为阿里云的镜像源，例如：\n\n```gradle\nmaven { url 'https://maven.aliyun.com/repository/google' }\nmaven { url 'https://maven.aliyun.com/repository/jcenter' }\n```\n\n这将帮助解决获取SDK和Gradle编译环境资源的问题。",
    "retrieve_time": 0.6168322563171387,
    "generate_time": 3.5959057807922363,
    "total_time": 4.212738037109375,
    "tokens_out": 123,
    "tokens_in": 208,
    "ground_truth": "google() 和 jcenter()",
    "context": [
      {
        "content": "因为获取SDK和gradle编译环境等资源，需要先给Android Studio配置proxy或者使用国内的镜像。可将 build.gradle 中的maven源 google() 和 jcenter() 分别替换为国内镜像，如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第425段"
      },
      {
        "content": "能配置build.gradle构建项目",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第18段"
      },
      {
        "content": "配置build.gradle",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第411段"
      },
      {
        "content": "maven { url ' https://maven.aliyun.com/repository/jcenter '}",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第429段"
      }
    ]
  },
  {
    "id": "q69",
    "model": "yi-6b-chat",
    "query": "TensorFlow Lite 提供的 Android 示例项目中，花卉识别代码所在的 Git 路径是什么？",
    "answer": "花卉识别代码所在的 Git 路径是：\n\n```\nexamples/lite/codelabs/flower_classification/android/\n```",
    "retrieve_time": 0.15556931495666504,
    "generate_time": 1.7377264499664307,
    "total_time": 1.8932957649230957,
    "tokens_out": 34,
    "tokens_in": 275,
    "ground_truth": "examples/lite/codelabs/flower_classification/android/",
    "context": [
      {
        "content": "项目代码位于目录examples/lite/codelabs/flower_classification/android/，start目录下为项目模板，finish目录下是项目完整代码。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第402段"
      },
      {
        "content": "工作目录中选择 examples/lite/codelabs/flower_classification/android/finish。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第409段"
      },
      {
        "content": "我们已经使用MobileNet V2 创建、训练和导出了自定义TensorFlow Lite模型，已经导出以下经过训练的TF Lite模型文件和标签文件。接下来将在手机端部署，运行一个使用该模型识别花卉图片的Android 应用。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第398段"
      },
      {
        "content": "下面将使用TensorFlow Lite实现花卉识别app，在Android设备上运行图像识别模型MobileNets_v2来识别花卉。本项目实施步骤如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第216段"
      }
    ]
  },
  {
    "id": "q70",
    "model": "yi-6b-chat",
    "query": "ImageDataGenerator 在示例中进行了哪两种主要功能？",
    "answer": "ImageDataGenerator 在示例中主要进行了以下两种功能：\n\n1. 像素缩放和图像数据操作。\n2. 数据增强，生成一个批次一个批次的图片用于模型训练。\n\n这些功能通过构造函数的参数进行配置，用于加载和预处理图像数据。",
    "retrieve_time": 0.14295697212219238,
    "generate_time": 2.148188829421997,
    "total_time": 2.2911458015441895,
    "tokens_out": 68,
    "tokens_in": 237,
    "ground_truth": "像素归一化 (rescale) 与验证集拆分 (validation_split)",
    "context": [
      {
        "content": "ImageDataGenerator的构造函数包含许多参数，用于指定加载后如何操作图像数据，包括像素缩放和数据增强。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第254段"
      },
      {
        "content": "Image Data Generator的构造罕述",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 186.4-189.0s"
      },
      {
        "content": "Kirons提供了Image Data Generator类",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 169.6-173.0s"
      },
      {
        "content": "将数据集划分为训练集和验证集。训练前需要手动加载图像数据，完成包括遍历数据集的目录结构、加载图像数据以及返回输入和输出。可以使用 Keras 提供的 ImageDataGenerator 类，它是keras.preprocessing.image模块中的图片生成器，负责生成一个批次一个批次的图片，以生成器的形式给模型训练",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第253段"
      }
    ]
  },
  {
    "id": "q71",
    "model": "yi-6b-chat",
    "query": "在训练花卉模型时，使用了多少个 epoch？",
    "answer": "参考资料中未提及训练花卉模型时使用的 epoch 数量。因此，无法从提供的资料中确定使用了多少个 epoch。\n\n回答: 不知道",
    "retrieve_time": 0.1656351089477539,
    "generate_time": 1.5898540019989014,
    "total_time": 1.7554891109466553,
    "tokens_out": 35,
    "tokens_in": 150,
    "ground_truth": "10",
    "context": [
      {
        "content": "通过迁移学习实现花卉识别模型",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第217段"
      },
      {
        "content": "我们将使用带有标注的花会数据集",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 91.6-94.8s"
      },
      {
        "content": "8.4 任务2：实现花卉识别",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第215段"
      },
      {
        "content": "在训练之前先编译模型，损失函数使用类别交叉熵。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第320段"
      }
    ]
  },
  {
    "id": "q72",
    "model": "yi-6b-chat",
    "query": "微调阶段额外训练的 epoch 数被设置为多少？",
    "answer": "根据提供的参考资料，微调阶段额外训练的 epoch 数被设置为 5。\n\n参考资料内容 4 明确指出：epochs=5。",
    "retrieve_time": 0.15087366104125977,
    "generate_time": 1.70174241065979,
    "total_time": 1.8526160717010498,
    "tokens_out": 38,
    "tokens_in": 232,
    "ground_truth": "5",
    "context": [
      {
        "content": "通过微调进一步提高性能，调整预训练模型的顶层权重，以便模型学习特定于数据集的高级特征，当训练数据集很大并且非常类似于预训练模型训练的原始数据集时，通常建议使用此技术。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第227段"
      },
      {
        "content": "fine_tune_at = 100",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第362段"
      },
      {
        "content": "由于是通过迁移学习改造模型，所以不改变基础模型的各项参数变量，因为这样才能保留原来大规模训练的优势。使用model.trainable = False，设置在训练中，基础模型的各项参数变量不会被新的训练修改数据。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第305段"
      },
      {
        "content": "epochs=5,",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第379段"
      }
    ]
  },
  {
    "id": "q73",
    "model": "yi-6b-chat",
    "query": "MobileNet V2 微调前的非可训练参数主要来源于什么？",
    "answer": "回答: MobileNet V2 微调前的非可训练参数主要来源于预训练模型的基础层（base_model）权重。在设置 `model.trainable = False` 时，这些基础层的权重被冻结，不参与训练更新。这些层通常是模型中从预训练任务中学习到的特征提取层。",
    "retrieve_time": 0.16343069076538086,
    "generate_time": 2.2642221450805664,
    "total_time": 2.4276528358459473,
    "tokens_out": 75,
    "tokens_in": 231,
    "ground_truth": "预训练的 ImageNet 权重",
    "context": [
      {
        "content": "当微调MobileNet V2",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 471.0-473.0s"
      },
      {
        "content": "设置model.trainable = False参数后，训练期间将不更新预训练网络的权重，只在MobileNet V2基础模型上训练了几层。如果希望进一步提高性能的方法是训练预训练模型的顶层的权重以及刚添加的分类器的训练。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第352段"
      },
      {
        "content": "MobileNet V2模型网络",
        "source": "04 TensorFlow Lite实现花卉识别-1_222124.mp4 452.0-454.0s"
      },
      {
        "content": "取消冻结base_model，MobileNet V2模型网络一共155层，前100层仍设置为无法训练，然后重新编译模型，并恢复训练。使用低学习率编译模型，代码如下：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第369段"
      }
    ]
  },
  {
    "id": "q74",
    "model": "yi-6b-chat",
    "query": "Android 应用中，若要使用 GPU 加速，需要声明哪种权限或设置？",
    "answer": "根据提供的资料，要在Android应用中使用GPU加速，不需要声明特定权限，但需要确保设备启用了“开发者模式”和“USB调试”设置。这些设置是为了能够将应用从Android Studio加载到手机上，从而利用GPU加速。\n\n因此，回答如下：\n\n**回答:** 需要设置设备启用“开发者模式”和“USB调试”，无需声明额外权限。",
    "retrieve_time": 0.1605696678161621,
    "generate_time": 2.5159261226654053,
    "total_time": 2.6764957904815674,
    "tokens_out": 91,
    "tokens_in": 220,
    "ground_truth": "无需额外权限，只需在代码中添加 GpuDelegate",
    "context": [
      {
        "content": "如果手机有GPU， GPU 比 CPU 执行更快的浮点矩阵运算，速度提升能有显著效果。例如在有GPU加速的手机上运行MobileNet图像分类，模型运行速度可以提高 5.5 倍。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第189段"
      },
      {
        "content": "利用手机GPU加速模型运行",
        "source": "1112_222613.mp4 109.5-115.2s"
      },
      {
        "content": "如图在Android 设备上运行，设置手机启用“开发者模式”和“USB 调试”，否则无法将该应用从 Android Studio 加载到您的手机上。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第540段"
      },
      {
        "content": "3) 能够利用各种硬件加速。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第30段"
      }
    ]
  },
  {
    "id": "q75",
    "model": "yi-6b-chat",
    "query": "PoseNet Android 示例仓库的完整 GitHub URL 是什么？",
    "answer": "PoseNet Android 示例仓库的完整 GitHub URL 是:\n\n```\nhttps://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android\n```",
    "retrieve_time": 0.15756607055664062,
    "generate_time": 2.145179271697998,
    "total_time": 2.3027453422546387,
    "tokens_out": 49,
    "tokens_in": 239,
    "ground_truth": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android",
    "context": [
      {
        "content": "https://github.com/tensorflow/examples/tree/master/lite/examples/posenet/android。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第553段"
      },
      {
        "content": "从 PoseNet 库中调用estimateSinglePose()函数来获取Person对象。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第559段"
      },
      {
        "content": "该PoseNet示例应用程序功能是捕捉摄像头拍摄的帧，并实时覆盖图像上的关键点。应用程序对每张传入的摄像头图像执行以下操作：",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第556段"
      },
      {
        "content": "通过PoseNet模型实现人体姿势估计，PoseNet可以通过检测关键身体部位的位置来估计图像或者视频中的人体姿势。例如，该模型可以估计图像中人的手肘和/或膝盖位置。",
        "source": "upload-6617113664522196356-cp08-样章示例-TensorFlow Lite_154550.docx 第551段"
      }
    ]
  },
  {
    "id": "q76",
    "model": "yi-6b-chat",
    "query": "TensorFlow.js 提供哪两大 API 层以构建模型？",
    "answer": "TensorFlow.js 提供 Kore API 和 Layer API 两大 API 层以构建模型。",
    "retrieve_time": 0.1559298038482666,
    "generate_time": 1.3093280792236328,
    "total_time": 1.4652578830718994,
    "tokens_out": 22,
    "tokens_in": 173,
    "ground_truth": "Core API 与 Layers API",
    "context": [
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "也可以通过TF.model来创建Layer's model。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 63.5-68.6s"
      },
      {
        "content": "以下是使用TF.model API建立和上文相同模型的例子。",
        "source": "upload-1017771394655554015-03 TensorFlow.js框架(模型与内存管理）_171842.mp4 74.2-81.7s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q77",
    "model": "yi-6b-chat",
    "query": "在浏览器环境中，TensorFlow.js 主要依赖哪项技术进行 GPU 加速？",
    "answer": "在浏览器环境中，TensorFlow.js 主要依赖 GPU 硬件加速技术进行加速。具体来说，它是通过 WebGL 技术来利用 GPU 进行加速计算的。WebGL 允许在浏览器中执行高性能的图形运算，TensorFlow.js 利用这一能力来进行高效的机器学习模型推理。\n\n因此，答案是 TensorFlow.js 在浏览器中主要依赖 WebGL 进行 GPU 加速。",
    "retrieve_time": 0.14156317710876465,
    "generate_time": 2.788177251815796,
    "total_time": 2.9297404289245605,
    "tokens_out": 92,
    "tokens_in": 171,
    "ground_truth": "WebGL",
    "context": [
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  },
  {
    "id": "q78",
    "model": "yi-6b-chat",
    "query": "在 Node.js 环境中，若安装 CUDA，TensorFlow.js 可使用哪种后端进行加速？",
    "answer": "在 Node.js 环境中，如果安装了 CUDA，TensorFlow.js 可以使用 CUDA 后端进行硬件加速。",
    "retrieve_time": 0.16114544868469238,
    "generate_time": 1.6397101879119873,
    "total_time": 1.8008556365966797,
    "tokens_out": 29,
    "tokens_in": 173,
    "ground_truth": "CUDA GPU 后端",
    "context": [
      {
        "content": "在Note.js 环境中如果有CUDA 环境支持",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 89.0-94.0s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      }
    ]
  },
  {
    "id": "q79",
    "model": "yi-6b-chat",
    "query": "若要在 HTML 中直接加载 TensorFlow.js，应使用哪个标签引入脚本？",
    "answer": "要在 HTML 中直接加载 TensorFlow.js，应使用 `<script>` 标签引入脚本。具体代码示例如下：\n\n```html\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\"></script>\n```\n\n这将从 CDN 加载 TensorFlow.js 库，方便在浏览器中使用。",
    "retrieve_time": 0.18187594413757324,
    "generate_time": 3.2682368755340576,
    "total_time": 3.450112819671631,
    "tokens_out": 90,
    "tokens_in": 192,
    "ground_truth": "<script> 标签",
    "context": [
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      },
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Index.html 文件中,使用脚本代码在流暖器项目中或许Tensorflow.js 以及TFGS 当VAS。",
        "source": "upload-17308567376682724494-06 手写数字识别-1_174128.mp4 234.8-245.8s"
      },
      {
        "content": "那么Tenzeflow.js 可以使用硬件进行加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 99.0-103.0s"
      }
    ]
  },
  {
    "id": "q80",
    "model": "yi-6b-chat",
    "query": "TensorFlow.js 张量的两个核心属性分别是什么？",
    "answer": "根据提供的资料，没有明确列出TensorFlow.js张量的核心属性。因此，基于标准TensorFlow.js的知识，TensorFlow.js张量的两个核心属性通常是：\n\n1. **数据类型（dtype）**：表示张量中元素的数据类型，例如float32、int32、bool等。\n2. **形状（shape）**：表示张量的维度大小，即张量在每个维度上包含的元素数量。\n\n这些属性帮助定义和操作张量。如果需要更详细的信息，建议参考TensorFlow.js的官方文档。",
    "retrieve_time": 0.1774446964263916,
    "generate_time": 3.4451348781585693,
    "total_time": 3.622579574584961,
    "tokens_out": 128,
    "tokens_in": 171,
    "ground_truth": "shape（形状）和 rank（阶/维度）",
    "context": [
      {
        "content": "在流览器中加载Tenzeflow.js 最方便的办法",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 144.0-149.0s"
      },
      {
        "content": "Tenzeflow.js包含Kore API 与Layer API 两部门",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 10.0-17.0s"
      },
      {
        "content": "Tenzeflow.js 支持GPU硬件加速",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 84.0-88.0s"
      },
      {
        "content": "是在HTML中直接引用Tenzeflow.js 发布的NPM包中",
        "source": "upload-1262541090359221833-02 TensorFlow.js框架_170151.mp4 149.0-155.0s"
      }
    ]
  }
]