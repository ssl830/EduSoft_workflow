#### 5.1.2 对话模型选型报告

##### 1. 评估概述

##### 1.1 背景与目标
随着教育场景对智能问答与个性化辅导需求的快速增长，选择一款高效、稳定且成本可控的中文大模型成为平台建设的关键。本次评测旨在**定量**与**定性**地对比主流中文 LLM，在统一的 RAG（Retrieval-Augmented Generation）流水线下，找出最契合我校教学平台的模型方案。

##### 1.2 受测模型
| 模型 | 版本 / 来源 | 关键词 | 备注 |
|------|-------------|--------|------|
| glm-4 | 智谱 AI API | 轻量、响应快 | 
| DeepSeek-V3 | DeepSeek API | 高质量长文本 | 
| qwen-max | 阿里云 DashScope API | 代码示例丰富 |
| yi-34b-chat | 01.AI API | 高 Source Recall |

> *Baichuan2-13B-Chat* 因连续出现 404 与 429 错误，数据缺失，已在本轮分析中排除。

##### 1.3 测试数据与流程概览
- **数据集**：200 条来自《嵌入式 Linux 开发实践教程》的真实教学问答（概念 60% / 实操 40%）。
- **统一 RAG**：FAISS + 余弦召回 Top-k=4，确保检索阶段一致。
- **计量指标**：BERTScore-F1、Rouge-L、Source Recall、平均延迟、输出 Token 数。
- **执行环境**：Win 10 + Python 3.10，全部调用官方云 API，网络延迟已剔除。

---

#### 2. 定量评估结果

| 模型         | BERTScore-F1 ↑ | Source Recall ↑ | Rouge-L ↑ | 平均延迟(s) ↓ | 输出Token数 ↓ |
|:-------------|---------------:|----------------:|-----------:|--------------:|--------------:|
| glm-4  | 0.599 | 0.016 | 0.269 | 1.87 | 40 |
| deepseek-v3  | 0.560 | 0.018 | 0.182 | 6.34 | 52 |
| qwen-max | 0.548 | 0.024 | 0.234 | 5.97 | 74 |
| yi-34b-chat   | 0.579 | 0.026 | 0.248 | 2.72 | 68 |

> **解读**：glm-4 在语义质量 (BERTScore / Rouge-L) 与速度、成本三维度综合领先；yi-34b-chat 在利用检索片段 (Source Recall) 上表现最佳。

---

#### 3. 评估方法

##### 3.1 数据集
- **来源**：同上。
- **结构**：`{id, query, answer}`，每条均含标准答案以便自动对齐。

##### 3.2 流程细节
1. **Retrieval**：FAISS 余弦相似度，Top-4 片段。
2. **Generation**：拼接 Prompt → 调用对应模型 API。
3. **Metric Logging**：记录时长 & Token 数。
4. **Scoring**：计算 BERTScore、Rouge-L 与 Source Recall。

##### 3.3 指标释义
| 指标 | 含义 | 趋势 |
|------|------|------|
| BERTScore-F1 | 语义相似度，衡量答案要点覆盖 | 越大越好 |
| Rouge-L | 文本重叠度 (最长公共子序列) | 越大越好 |
| Source Recall | 回答中引用检索片段比例 | 越大越好 |
| 平均延迟 | 端到端耗时 (秒) | 越小越好 |
| 输出 Token 数 | 生成字数，映射调用成本 | 越小越好 |

---

#### 4. 结论与模型选择建议

##### 4.1 优势对比分析
| 场景 | 推荐模型 | 关键理由 |
|------|----------|-----------|
| 在线课堂 / 即问即答 | **glm-4** | 最高的 BERTScore 与 Rouge-L，延迟最优，Token 成本最低 |
| 离线或局域网私有部署 | **glm-4** 或 yi-34b-chat | 参数规模适中，显存需求 9-10 GB，可在单卡运行；Yi 在 Source Recall 上更佳 |
| 教学内容深度解析 | **yi-34b-chat** | 利用检索内容能力最强，回答信息量大 |
| 需要详细教程示例 | **DeepSeek-V3** | 生成解释与代码示例丰富，适合编程类教学 |

> 建议采用 **glm-4 + yi-34b-chat** 的双模型策略：默认使用 glm-4，遇到长篇理论性问题或首选模型超时失败时回退到 yi-34b-chat，可兼顾质量与稳定性。

##### 4.2 网站模型选择的实际设计

> **目标**：将评估结论真正落地到生产环境，既要保证最佳体验，又要具备容灾与可维护性。

##### 4.2.1 架构概览
1. **统一入口 `ModelSelector`**  （见 `ai_service/services/model_selector.py`）  
   - 封装所有 LLM 调用，业务代码完全解耦。  
   - 依赖 [OpenAI python-sdk](https://github.com/openai/openai-python) 的兼容层，所有云厂商统一成 `client.chat.completions.create()` 接口，减少学习成本。
2. **优先级 & 回退机制**  
   - 默认顺序：`glm-4 → yi-34b-chat → deepseek-v3`，与 2中的定量得分一致。  
   - 每次请求若 ①超时、②HTTP 4xx/5xx、③触发余额限制，则自动切换到下一个可用模型。
3. **无状态设计**  
   - ModelSelector 不持有会话信息，方便水平扩缩。  
   - 会话上下文由调用层（FastAPI Session / Vue 页面）负责传入。
4. **环境变量驱动的动态配置**  
   - Dev / Prod 共用同一代码，仅通过 `.env` 指定 `CHATGLM_API_KEY` / `YI_API_KEY` 等即可启用或禁用对应后端。

##### 4.2.3 关键代码片段
```python
# ai_service/services/model_selector.py (节选)
class ModelSelector:
    def chat_completion(self, *, messages: list[dict], temperature: float = 0.7, **extra):
        for key in self.priority:              # 按优先级依次尝试
            try:
                response = self._clients[key]["client"].chat.completions.create(
                    model=self._clients[key]["model_name"],
                    messages=messages,
                    temperature=temperature,
                    **extra,
                )
                return response               # 命中直接返回
            except Exception as exc:          # 捕获所有异常并回退
                logger.warning("%s failed: %s", key, exc)
        raise RuntimeError("All backends failed")
```

##### 4.2.4 请求流程示意
```mermaid
sequenceDiagram
    participant FE as Frontend (Vue)
    participant API as Backend (FastAPI)
    participant MS as ModelSelector
    participant LLM1 as glm-4
    participant LLM2 as yi-34b-chat

    FE->>API: POST /assistant/query
    API->>MS: assemble messages[]
    MS->>LLM1: chat.completions.create()
    alt Success
        LLM1-->>MS: response
        MS-->>API: content
    else Error / Timeout
        MS->>LLM2: chat.completions.create()
        LLM2-->>MS: response
        MS-->>API: content
    end
    API-->>FE: JSON (answer, usage, model)
```

##### 4.2.5 运维与监控
- **日志**：每次调用记录 `model`, `latency`, `tokens`，写入 ELK；错误堆栈附带 `key` 方便告警定位。  
- **Prometheus 指标**：`llm_latency_seconds`, `llm_fallback_total` 等，用于 Grafana 看板；一旦 fallback 比例 > 5% 则触发预警。
- **灰度发布**：新模型加入只需在 `.env` 中写入 API Key，并上调 `PRIMARY_LLM_MODEL`；Prometheus 自动对比延迟分位数。

##### 4.2.6 与 RAG 流水线的集成
```
retrieve_ctx = faiss_index.search(query, top_k=4)
messages = build_messages(query, retrieve_ctx)
answer = ModelSelector().chat_completion(messages=messages).choices[0].message.content
```
- **Prompt 统一**：无论切到哪家模型，都使用相同 system & user prompt，保证对比公平。  
- **Source Recall 强化**：针对 Yi 模型，我们在 prompt 中增加 `"请引用检索内容回答"`，配合其高 recall 特性。

> **总结**：通过模块化的 ModelSelector + 完整的监控链路，我们不仅在评测阶段做到了量化对比，更在工程落地中确保了**高可用**、**低成本**、**可演进**。

---

#### 5. 详细分析

##### 5.1 响应延迟对比

![响应延迟对比](./image/image-105.png)

##### 5.2 回答质量对比

![回答质量对比](./image/image-106.png)

#### 5.3 典型问题回答样本

##### 5.3.1 问题: TensorFlow Lite 主要面向哪三类设备进行模型部署？

**参考答案**: 移动设备（Android、iOS）、嵌入式设备以及微控制器（MCU）

**glm-4回答**:
TensorFlow Lite 主要面向移动设备、嵌入式设备和物联网（IoT）设备进行模型部署。

**deepseek-v3回答**:
根据参考资料，TensorFlow Lite 主要面向以下三类设备进行模型部署：  
1. **移动设备**（如 Android、iOS 设备）  
2. **嵌入式设备**  
3. **物联网（IoT）设备**（包括极小的 MCU 设备）  

（依据：内容1、内容2 和 内容4 中均明确提到这三类设备。）

**qwen-max回答**:
TensorFlow Lite 主要面向三类设备进行模型部署：移动设备（如智能手机）、嵌入式设备以及物联网(IoT)设备。

**yi-34b-chat回答**:
TensorFlow Lite 主要面向移动设备（mobile）、嵌入式设备（embedded）和物联网设备（IoT）进行模型部署。

##### 5.3.2 问题: 在 Python 中将 SavedModel 转换为 TFLite 模型应使用哪个类？

**参考答案**: tf.lite.TFLiteConverter

**glm-4回答**:
应使用 `tf.lite.TFLiteConverter` 类，并通过调用其 `from_saved_model()` 方法来进行转换。

**deepseek-v3回答**:
在 Python 中将 SavedModel 转换为 TFLite 模型应使用 `tf.lite.TFLiteConverter` 类，具体调用其 `from_saved_model()` 方法。示例代码如下：

```python
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
```

其中 `saved_model_dir` 是 SavedModel 的目录路径。

**qwen-max回答**:
在 Python 中将 SavedModel 转换为 TFLite 模型应使用 `tf.lite.TFLiteConverter` 类。具体来说，可以通过调用 `TFLiteConverter.from_saved_model()` 方法，并传入包含SavedModel的目录路径来创建转换器实例。例如：

```python
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
```

这里的 `saved_model_dir` 是指向你的SavedModel所在目录的路径。

**yi-34b-chat回答**:
在 Python 中将 SavedModel 转换为 TFLite 模型应使用 `tf.lite.TFLiteConverter` 类，并调用其 `from_saved_model()` 方法。

示例代码：
```python
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
```

其中 `saved_model_dir` 是 SavedModel 文件目录的路径。

##### 5.3.3 问题: TensorFlow.js 支持在浏览器调用手机哪些硬件传感器？请列举至少一个。

**参考答案**: GPS、摄像头、陀螺仪等

**glm-4回答**:
根据提供的资料，TensorFlow.js 可以调用手机硬件的传感器。但是具体支持哪些硬件传感器，资料中并没有明确列出。因此，我不能准确地列举出至少一个支持的硬件传感器。不知道。

**deepseek-v3回答**:
根据参考资料，TensorFlow.js 支持使用硬件进行加速（内容2），但未明确提及具体支持调用的手机硬件传感器类型。因此，目前无法从给定资料中确定 TensorFlow.js 支持的手机传感器。

**qwen-max回答**:
根据提供的资料，没有具体提到TensorFlow.js可以直接调用手机的哪些硬件传感器。因此，对于您的问题，我没有足够的信息来列举TensorFlow.js可以在浏览器中直接调用的手机硬件传感器。不过，一般而言，通过JavaScript和Web API，可以访问到某些传感器的数据（如加速度计、陀螺仪等），但这些功能并非直接由TensorFlow.js提供支持。若要实现与传感器数据相关的机器学习应用，可能需要结合使用Web API获取传感器数据以及TensorFlow.js进行模型推理。

**yi-34b-chat回答**:
根据提供的资料，TensorFlow.js 可以在浏览器中调用手机硬件进行加速，但并没有详细列举具体支持的传感器类型。不过，结合已知信息和实际应用情况，TensorFlow.js 可以通过浏览器访问和利用手机的运动传感器，例如**加速度计**。

因此，答案为：
TensorFlow.js 支持在浏览器中调用手机的加速度计传感器。
